{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c408f133-3f66-4adf-85cc-89cc563d610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\") #See if your kernel crasher or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6e46b3-92fa-43c3-b5d8-e82a63b29bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 60.19921875 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def check_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Memory Usage: {process.memory_info().rss / (1024 * 1024)} MB\")\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07d64481-5dad-447c-8223-e4f4bd8fd7ac",
   "metadata": {},
   "source": [
    "I am checking the memory usage as the kernel is dying in the big code block. I am trying the packages individually to see if one of them is causing the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e97cd47-218f-4383-b8ca-41216caa4f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n'NoneType' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\image_processing_utils.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\image_transforms.py:56\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_v2_available():\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_torchvision_available():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoAugmentPolicy, InterpolationMode  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\transforms\\v2\\functional\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_pure_tensor, register_kernel  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_meta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     clamp_bounding_boxes,\n\u001b[0;32m      7\u001b[0m     convert_bounding_box_format,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     get_size,\n\u001b[0;32m     22\u001b[0m )  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\transforms\\v2\\functional\\_utils.py:5\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tv_tensors\n\u001b[0;32m      7\u001b[0m _FillType \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Sequence[\u001b[38;5;28mint\u001b[39m], Sequence[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\models\\convnext.py:9\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoolers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_align\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroi_align\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torchvision\\ops\\roi_align.py:7\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:33\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\exc.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\utils.py:1752\u001b[0m\n\u001b[0;32m   1730\u001b[0m common_constant_types: Set[\u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1731\u001b[0m     \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_CudaDeviceProperties,\n\u001b[0;32m   1750\u001b[0m }\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_triton_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\utils\\_triton.py:9\u001b[0m, in \u001b[0;36mhas_triton_package\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m triton_key\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m triton_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\__init__.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindows_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_cuda, find_msvc_winsdk\n\u001b[1;32m----> 8\u001b[0m msvc_winsdk_inc_dirs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfind_msvc_winsdk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msvc_winsdk_inc_dirs:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\windows_utils.py:230\u001b[0m, in \u001b[0;36mfind_msvc_winsdk\u001b[1;34m()\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_msvc_winsdk\u001b[39m():\n\u001b[1;32m--> 230\u001b[0m     msvc_inc_dirs, msvc_lib_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mfind_msvc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     winsdk_inc_dirs, winsdk_lib_dirs \u001b[38;5;241m=\u001b[39m find_winsdk()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\windows_utils.py:146\u001b[0m, in \u001b[0;36mfind_msvc\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msvc_base_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     msvc_base_path, version \u001b[38;5;241m=\u001b[39m \u001b[43mfind_msvc_hardcoded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msvc_base_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\windows_utils.py:123\u001b[0m, in \u001b[0;36mfind_msvc_hardcoded\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m vs_path \u001b[38;5;241m=\u001b[39m find_in_program_files(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMicrosoft Visual Studio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvs_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m():\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'exists'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m test_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n'NoneType' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "test_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\", device=0)\n",
    "print(test_pipeline(\"Hello, how are you?\", max_length=20, num_return_sequences=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589883e8-ac82-49fd-8459-07037fa045ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import pipeline\n",
    "from datasketch import MinHashLSHForest, MinHash\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Keyword Extraction\n",
    "def extract_keywords(question):\n",
    "    \"\"\"\n",
    "    Extract keywords from a natural language question using a smaller LLM.\n",
    "    \"\"\"\n",
    "    # Load a smaller LLM for keyword extraction\n",
    "    keyword_extractor = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    \n",
    "    # Generate keywords\n",
    "    prompt = f\"Extract keywords from the following question: {question}\"\n",
    "    keywords = keyword_extractor(prompt, max_length=50, num_return_sequences=1)\n",
    "    return keywords[0]['generated_text'].split()  # Simple split for demo purposes\n",
    "\n",
    "# Step 2: Locality-Sensitive Hashing (LSH)\n",
    "def create_lsh_forest(database_words):\n",
    "    \"\"\"\n",
    "    Create an LSH Forest and add database words to it.\n",
    "    \"\"\"\n",
    "    forest = MinHashLSHForest(num_perm=128)\n",
    "    \n",
    "    for i, word in enumerate(database_words):\n",
    "        m = MinHash(num_perm=128)\n",
    "        for char in word:\n",
    "            m.update(char.encode('utf-8'))\n",
    "        forest.add(i, m)\n",
    "    \n",
    "    # Index the forest\n",
    "    forest.index()\n",
    "    return forest\n",
    "\n",
    "def find_similar_words(query_word, forest, database_words, top_k=5):\n",
    "    \"\"\"\n",
    "    Find similar words in the database using LSH.\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=32)\n",
    "    for char in query_word:\n",
    "        m.update(char.encode('utf-8'))\n",
    "    results = forest.query(m, top_k)\n",
    "    return [database_words[i] for i in results]\n",
    "\n",
    "# Step 3: Re-ranking\n",
    "def re_rank_words(query_word, similar_words, word_embeddings):\n",
    "    \"\"\"\n",
    "    Re-rank similar words based on embedding similarity and edit distance.\n",
    "    \"\"\"\n",
    "    query_embedding = word_embeddings[query_word]\n",
    "    scores = []\n",
    "    \n",
    "    for word in similar_words:\n",
    "        # Cosine similarity\n",
    "        cosine_sim = cosine_similarity([query_embedding], [word_embeddings[word]])[0][0]\n",
    "        # Edit distance\n",
    "        edit_dist = levenshtein_distance(query_word, word)\n",
    "        # Combined score (higher is better)\n",
    "        combined_score = cosine_sim - (edit_dist / 10)  # Adjust weights as needed\n",
    "        scores.append((word, combined_score))\n",
    "    \n",
    "    # Sort by combined score\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, _ in scores]\n",
    "\n",
    "# Step 4: SQL Query Generation\n",
    "def generate_sql(keywords, re_ranked_words):\n",
    "    \"\"\"\n",
    "    Generate an SQL query using extracted keywords and re-ranked words.\n",
    "    \"\"\"\n",
    "    table = \"customers\"  # Replace with your table name\n",
    "    columns = [\"customer_id\", \"product\", \"year\"]  # Replace with your columns\n",
    "    sql = f\"SELECT * FROM {table} WHERE \"\n",
    "    conditions = []\n",
    "    \n",
    "    for keyword, word in zip(keywords, re_ranked_words):\n",
    "        if keyword in columns:\n",
    "            conditions.append(f\"{keyword} = '{word}'\")\n",
    "    \n",
    "    sql += \" AND \".join(conditions)\n",
    "    return sql\n",
    "\n",
    "# Main Pipeline\n",
    "def text_to_sql_pipeline(question, database_words, word_embeddings):\n",
    "    \"\"\"\n",
    "    Full pipeline to convert a natural language question into an SQL query.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract keywords\n",
    "    keywords = extract_keywords(question)\n",
    "    print(\"Extracted Keywords:\", keywords)\n",
    "    \n",
    "    # Step 2: Create LSH Forest and find similar words\n",
    "    forest = create_lsh_forest(database_words)\n",
    "    similar_words = []\n",
    "    for keyword in keywords:\n",
    "        similar_words.extend(find_similar_words(keyword, forest, database_words))\n",
    "    print(\"Similar Words:\", similar_words)\n",
    "    \n",
    "    # Step 3: Re-rank words\n",
    "    re_ranked_words = []\n",
    "    for keyword in keywords:\n",
    "        re_ranked_words.extend(re_rank_words(keyword, similar_words, word_embeddings))\n",
    "    print(\"Re-ranked Words:\", re_ranked_words)\n",
    "    \n",
    "    # Step 4: Generate SQL query\n",
    "    sql_query = generate_sql(keywords, re_ranked_words)\n",
    "    return sql_query\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example database words (replace with your actual database words)\n",
    "    database_words = [\"customer\", \"product\", \"2023\", \"purchase\", \"order\", \"year\"]\n",
    "    \n",
    "    # Example word embeddings (replace with actual embeddings from your LLM)\n",
    "    word_embeddings = {\n",
    "        \"customer\": np.random.rand(32),\n",
    "        \"product\": np.random.rand(32),\n",
    "        \"2023\": np.random.rand(32),\n",
    "        \"purchase\": np.random.rand(32),\n",
    "        \"order\": np.random.rand(32),\n",
    "        \"year\": np.random.rand(32),\n",
    "    }\n",
    "    \n",
    "    # Example question\n",
    "    question = \"Find all customers who bought products in 2023.\"\n",
    "    \n",
    "    # Run the pipeline\n",
    "    sql_query = text_to_sql_pipeline(question, database_words, word_embeddings)\n",
    "    print(\"Generated SQL Query:\", sql_query)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58620679-8717-439f-ba18-4f22c8edd8ba",
   "metadata": {},
   "source": [
    "New try as previous code did not work, now llama 3.2 is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc488e1b-1572-42e9-beca-03338aee831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure ollama for python is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e400f37-b379-4dc3-a967-5497b8c12f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.0)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 15.8 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, annotated-types, pydantic-core, pydantic, ollama\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.4.7 pydantic-2.10.6 pydantic-core-2.27.2 typing-extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu118 requires torch==2.6.0+cu118, but you have torch 2.5.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "#having an older torch audio like 2.5.1 should not affect much. Even thhough ollama recommend 2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221c4608-10dc-4a88-85b2-a3d31d9aa7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423ba183-bc4f-43b0-ac00-282c09561f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: Here are the extracted keywords:\n",
      "\n",
      "1. Technology\n",
      "2. Stocks\n",
      "3. Top-performing\n",
      "4. Sector\n",
      "5. 2024\n"
     ]
    }
   ],
   "source": [
    "# Using Ollama to run Llama3.2 for keyword extraction as first try for the few shot examples\n",
    "def extract_keywords(text):\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Extract keywords from this text: '{text}'\"}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example input\n",
    "question = \"What are the top-performing stocks in the technology sector for 2024?\"\n",
    "keywords = extract_keywords(question)\n",
    "print(\"Extracted Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc4d93f0-8f24-4b60-8303-c353993cd3df",
   "metadata": {},
   "source": [
    "Wonderful, now Ollama 3.2 is capable of extraction keywords out of a sentence. Next step is to download the SPIDER dataset, we will do it by installing datasets and the using the function load_dataset(\"spider\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df97d27f-808e-45ae-85dc-4115cfb48ca3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#SPIDER dataset from Hugging Face download\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCM/spider\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\datasets\\arrow_dataset.py:59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\pandas\\__init__.py:23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401,E501\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\pandas\\compat\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     pa_version_under7p0,\n\u001b[0;32m     29\u001b[0m     pa_version_under8p0,\n\u001b[0;32m     30\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     31\u001b[0m     pa_version_under11p0,\n\u001b[0;32m     32\u001b[0m     pa_version_under13p0,\n\u001b[0;32m     33\u001b[0m     pa_version_under14p0,\n\u001b[0;32m     34\u001b[0m     pa_version_under14p1,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\pandas\\compat\\pyarrow.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(Version(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\u001b[38;5;241m.\u001b[39mbase_version)\n\u001b[0;32m     11\u001b[0m     pa_version_under7p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     pa_version_under8p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#SPIDER dataset from Hugging Face download\n",
    "dataset = load_dataset(\"CM/spider\")\n",
    "\n",
    "#preview\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5887c3f3-f81a-4a08-8fa2-694767a8a737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7282f3e0480645d4acaa9d859f64f2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow\n",
    "from datasets import load_dataset\n",
    "##I had to downgrade pyarrow to 12.01. as the version 19.0 had an error that prevented the from running. Still the prompt warned about datasets needing at least the pyarrow 1.5 version\n",
    "#Downgrading did not worked, then I noticed the dataset installation i had was old, and procced to update pyarrow and datasets\n",
    "ds = load_dataset(\"CM/spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a70050d-3e69-490a-90c7-bae4d13220f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['db_id', 'query', 'question', 'schema', 'query_res'],\n",
      "        num_rows: 7672\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['db_id', 'query', 'question', 'schema', 'query_res'],\n",
      "        num_rows: 665\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Okay this indicates that the data set was successfully downloaded\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b89feaa-25a2-41b3-a641-67feb3a3e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   db_id                                              query  \\\n",
      "0  department_management         SELECT count(*) FROM head WHERE age  >  56   \n",
      "1  department_management  SELECT name ,  born_state ,  age FROM head ORD...   \n",
      "2  department_management  SELECT creation ,  name ,  budget_in_billions ...   \n",
      "3  department_management  SELECT max(budget_in_billions) ,  min(budget_i...   \n",
      "4  department_management  SELECT avg(num_employees) FROM department WHER...   \n",
      "5  department_management  SELECT name FROM head WHERE born_state != 'Cal...   \n",
      "6  department_management  SELECT DISTINCT T1.creation FROM department AS...   \n",
      "7  department_management  SELECT born_state FROM head GROUP BY born_stat...   \n",
      "8  department_management  SELECT creation FROM department GROUP BY creat...   \n",
      "9  department_management  SELECT T1.name ,  T1.num_employees FROM depart...   \n",
      "\n",
      "                                            question  \\\n",
      "0  How many heads of the departments are older th...   \n",
      "1  List the name, born state and age of the heads...   \n",
      "2  List the creation year, name and budget of eac...   \n",
      "3  What are the maximum and minimum budget of the...   \n",
      "4  What is the average number of employees of the...   \n",
      "5  What are the names of the heads who are born o...   \n",
      "6  What are the distinct creation years of the de...   \n",
      "7  What are the names of the states where at leas...   \n",
      "8   In which year were most departments established?   \n",
      "9  Show the name and number of employees for the ...   \n",
      "\n",
      "                                              schema  \\\n",
      "0  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "1  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "2  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "3  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "4  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "5  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "6  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "7  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "8  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "9  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "\n",
      "                                           query_res  \n",
      "0                                             (5,)\\n  \n",
      "1  ('Pádraig Harrington', 'Connecticut', 43.0)\\n(...  \n",
      "2  ('1789', 'State', 9.96)\\n('1789', 'Treasury', ...  \n",
      "3                                     (543.2, 6.2)\\n  \n",
      "4                            (105468.16666666667,)\\n  \n",
      "5  ('Tiger Woods',)\\n('K. J. Choi',)\\n('Jeff Magg...  \n",
      "6                                        ('1903',)\\n  \n",
      "7                                  ('California',)\\n  \n",
      "8                                        ('1789',)\\n  \n",
      "9  ('Treasury', 115897.0)\\n('Homeland Security', ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the training set to a pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(df_train.head(10))\n",
    "\n",
    "#As visualized in the huggingface dataset viewer, the dataset has 6 features(or columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b54b706-6fbd-4f07-bf18-aba22239efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"creation\", \"years\", \"departments\"]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "def extract_keywords(text):\n",
    "    few_shot_examples = \"\"\"Extract keywords from the following questions. Examples:\n",
    "    \n",
    "    Question: \"How many heads of the departments are older than 56?\"\n",
    "    Keywords: [\"heads\", \"departments\", \"older\", \"56\"]\n",
    "    \n",
    "    Question: \"List the name, born state, and age of the heads of departments ordered by age.\"\n",
    "    Keywords: [\"name\", \"born state\", \"age\", \"heads\", \"departments\", \"ordered\", \"age\"]\n",
    "    \n",
    "    Question: \"What is the average number of employees of the departments where budget is over 1 billion?\"\n",
    "    Keywords: [\"average\", \"number of employees\", \"departments\", \"budget\", \"over\", \"1 billion\"]\n",
    "    \n",
    "    Now extract keywords from this new question:\n",
    "    Question: \"{text}\"\n",
    "    Keywords:\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": few_shot_examples.format(text=text)}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Test with an example from the dataset\n",
    "question = \"What are the distinct creation years of the departments?\"\n",
    "keywords = extract_keywords(question)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aab25a8a-9c08-43d1-86ea-5ec7c1d8f72d",
   "metadata": {},
   "source": [
    "In the few shots examples I plan to add some examples of other categories of questions according to the different questions present in the spider dataset. First lets list them all using pandas functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539c0cf3-2078-47bf-879c-e98ec21e08a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['department_management', 'farm', 'student_assessment', 'bike_1', 'book_2', 'musical', 'product_catalog', 'flight_1', 'allergy_1', 'store_1', 'journal_committee', 'customers_card_transactions', 'race_track', 'coffee_shop', 'insurance_fnol', 'medicine_enzyme_interaction', 'university_basketball', 'phone_1', 'match_season', 'climbing', 'body_builder', 'election_representative', 'apartment_rentals', 'game_injury', 'soccer_1', 'performance_attendance', 'debate', 'insurance_and_eClaims', 'customers_and_invoices', 'wedding', 'theme_gallery', 'riding_club', 'gymnast', 'browser_web', 'wrestler', 'school_finance', 'protein_institute', 'cinema', 'products_for_hire', 'phone_market', 'gas_company', 'party_people', 'pilot_record', 'cre_Doc_Control_Systems', 'local_govt_in_alabama', 'machine_repair', 'entrepreneur', 'perpetrator', 'csu_1', 'candidate_poll', 'movie_1', 'county_public_safety', 'local_govt_mdm', 'party_host', 'storm_record', 'election', 'news_report', 'restaurant_1', 'customer_deliveries', 'sakila_1', 'loan_1', 'behavior_monitoring', 'assets_maintenance', 'station_weather', 'sports_competition', 'manufacturer', 'hr_1', 'music_1', 'baseball_1', 'mountain_photos', 'program_share', 'e_learning', 'insurance_policies', 'hospital_1', 'ship_mission', 'company_employee', 'film_rank', 'cre_Doc_Tracking_DB', 'club_1', 'tracking_grants_for_research', 'network_2', 'decoration_competition', 'document_management', 'company_office', 'solvency_ii', 'entertainment_awards', 'customers_campaigns_ecommerce', 'college_3', 'department_store', 'aircraft', 'local_govt_and_lot', 'school_player', 'store_product', 'soccer_2', 'device', 'cre_Drama_Workshop_Groups', 'music_2', 'manufactory_1', 'tracking_software_problems', 'shop_membership', 'voter_2', 'products_gen_characteristics', 'swimming', 'railway', 'customers_and_products_contacts', 'dorm_1', 'customer_complaints', 'workshop_paper', 'tracking_share_transactions', 'cre_Theme_park', 'game_1', 'customers_and_addresses', 'music_4', 'roller_coaster', 'ship_1', 'city_record', 'e_government', 'school_bus', 'flight_company', 'cre_Docs_and_Epenses', 'scientist_1', 'train_station', 'driving_school', 'activity_1', 'tracking_orders', 'architecture', 'culture_company', 'geo', 'scholar', 'yelp', 'academic', 'imdb', 'restaurants']\n"
     ]
    }
   ],
   "source": [
    "unique_db_ids = ds[\"train\"].unique(\"db_id\")\n",
    "print(unique_db_ids)\n",
    "#this will list all unique entries in db_id column, remember that is necessary to either use the train or test split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af4833fa-2fa8-40c5-b290-15db30005b8a",
   "metadata": {},
   "source": [
    "As it can be seem there are many different themes, so in order to balance it, I will take 4 examples of each topic to have a \"balanced\" few shots. This lacks of any source, so it is more of a gut feeling. It might be worth looking into it further, if it makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06dfcc0-f669-4e23-bcf9-e1e2dc45805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot examples saved. Here’s a preview:\n",
      "\n",
      "Question: \"return me the authors who have papers in VLDB conference before 2002 .\"\n",
      "Keywords: Keywords:\n",
      "\n",
      "[\"authors\", \"papers\", \"VLDB\", \"conference\", \"before\", \"2002\"]\n",
      "\n",
      "Question: \"return me all the papers, which contain the keyword \" Natural Language \" .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"papers\", \"keyword\", \"Natural Language\"]\n",
      "\n",
      "Question: \"return me the author in the \" University of Michigan \" whose papers have the most total citations .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "Keywords: [\"author\", \"University of Michigan\", \"papers\", \"total citations\"]\n",
      "\n",
      "Question: \"return me the number of papers on VLDB conference .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"papers\", \"VLDB conference\", \"number\", \"return\"]\n",
      "\n",
      "Question: \"What is the first and last name of the student participating in the most activities?\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "Keywords: [\"student\", \"participating in\", \"most\", \"activities\"]\n",
      "\n",
      "Question: \"How many faculty do we have?\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"faculty\", \"we\", \"have\"]\n",
      "\n",
      "Question: \"For each faculty rank, show the number of faculty members who have it.\"\n",
      "Keywords: Keywords: [\"faculty\", \"rank\", \"show\", \"number\", \"members\"]\n",
      "\n",
      "Question: \"Which building has most faculty members?\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"building\", \"faculty\", \"members\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "\n",
    "# Function to extract keywords using an LLM\n",
    "def extract_keywords(text):\n",
    "    few_shot_prompt = \"\"\"Extract keywords from the following questions. Examples:\n",
    "    \n",
    "    Question: \"How many heads of the departments are older than 56?\"\n",
    "    Keywords: [\"heads\", \"departments\", \"older\", \"56\"]\n",
    "    \n",
    "    Question: \"List the name, born state, and age of the heads of departments ordered by age.\"\n",
    "    Keywords: [\"name\", \"born state\", \"age\", \"heads\", \"departments\", \"ordered\", \"age\"]\n",
    "    \n",
    "    Question: \"What is the average number of employees of the departments where budget is over 1 billion?\"\n",
    "    Keywords: [\"average\", \"number of employees\", \"departments\", \"budget\", \"over\", \"1 billion\"]\n",
    "    \n",
    "    Now extract keywords from this new question:\n",
    "    Question: \"{text}\"\n",
    "    Keywords:\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{\"role\": \"user\", \"content\": few_shot_prompt.format(text=text)}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "\n",
    "# Group dataset by db_id\n",
    "grouped = df_train.groupby(\"db_id\")\n",
    "\n",
    "few_shot_examples = []\n",
    "\n",
    "# Loop through each topic\n",
    "for db_id, group in grouped:\n",
    "    sampled_questions = group.sample(n=min(4, len(group)), random_state=42)\n",
    "\n",
    "    for _, row in sampled_questions.iterrows():\n",
    "        # Extract keywords using LLM\n",
    "        extracted_keywords = extract_keywords(row[\"question\"])\n",
    "\n",
    "        # Store formatted example\n",
    "        few_shot_examples.append(\n",
    "            f'Question: \"{row[\"question\"]}\"\\nKeywords: {extracted_keywords}\\n'\n",
    "        )\n",
    "\n",
    "# Save the examples\n",
    "with open(\"few_shot_prompt.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(few_shot_examples))\n",
    "\n",
    "print(\"Few-shot examples saved. Here’s a preview:\\n\")\n",
    "print(\"\\n\".join(few_shot_examples[:8]))  # Show first 8 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa8647b-f441-4b5d-9c00-b9adcda65b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_id\n",
      "academic             181\n",
      "activity_1            88\n",
      "aircraft              46\n",
      "allergy_1             98\n",
      "apartment_rentals     80\n",
      "                    ... \n",
      "voter_2               72\n",
      "wedding               20\n",
      "workshop_paper        30\n",
      "wrestler              40\n",
      "yelp                 111\n",
      "Name: question, Length: 133, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(grouped[\"question\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70ac9897-98aa-4b59-a5d2-c427fb884ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n'NoneType' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\modeling_utils.py:49\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attention_forward\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flex_attention_forward\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdpa_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdpa_attention_forward\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\integrations\\flex_attention.py:9\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_flex_attn_available():\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flex_attention\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflex_attention_forward\u001b[39m(\n\u001b[0;32m     13\u001b[0m     module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m     14\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     21\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\nn\\attention\\flex_attention.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trace_wrapped_higher_order_op\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformGetItemToIndex\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flex_attention \u001b[38;5;28;01mas\u001b[39;00m flex_attention_hop\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:33\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\exc.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\_dynamo\\utils.py:1752\u001b[0m\n\u001b[0;32m   1730\u001b[0m common_constant_types: Set[\u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1731\u001b[0m     \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_CudaDeviceProperties,\n\u001b[0;32m   1750\u001b[0m }\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_triton_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\torch\\utils\\_triton.py:9\u001b[0m, in \u001b[0;36mhas_triton_package\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m triton_key\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m triton_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\__init__.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindows_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_cuda, find_msvc_winsdk\n\u001b[1;32m----> 8\u001b[0m msvc_winsdk_inc_dirs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfind_msvc_winsdk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msvc_winsdk_inc_dirs:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\windows_utils.py:230\u001b[0m, in \u001b[0;36mfind_msvc_winsdk\u001b[1;34m()\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_msvc_winsdk\u001b[39m():\n\u001b[1;32m--> 230\u001b[0m     msvc_inc_dirs, msvc_lib_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mfind_msvc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     winsdk_inc_dirs, winsdk_lib_dirs \u001b[38;5;241m=\u001b[39m find_winsdk()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\windows_utils.py:146\u001b[0m, in \u001b[0;36mfind_msvc\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msvc_base_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     msvc_base_path, version \u001b[38;5;241m=\u001b[39m \u001b[43mfind_msvc_hardcoded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msvc_base_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\triton\\windows_utils.py:123\u001b[0m, in \u001b[0;36mfind_msvc_hardcoded\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m vs_path \u001b[38;5;241m=\u001b[39m find_in_program_files(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMicrosoft Visual Studio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvs_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m():\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'exists'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\integrations\\integration_utils.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n'NoneType' object has no attribute 'exists'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mLevenshtein\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasketch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinHash, MinHashLSH\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#list ot store the keywords\u001b[39;00m\n\u001b[0;32m     10\u001b[0m keyword_store \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\__init__.py:14\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\evaluation\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMSEEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSEEvaluator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMSEEvaluatorFromDataFrame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSEEvaluatorFromDataFrame\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNanoBEIREvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NanoBEIREvaluator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParaphraseMiningEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParaphraseMiningEvaluator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRerankingEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RerankingEvaluator\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\evaluation\\NanoBEIREvaluator.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInformationRetrievalEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InformationRetrievalEvaluator\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\sentence_transformers\\model_card.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\huggingface_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n'NoneType' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ollama\n",
    "import random\n",
    "import Levenshtein\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#list ot store the keywords\n",
    "keyword_store = []\n",
    "\n",
    "# Loop through each topic\n",
    "for db_id, group in grouped:\n",
    "    sampled_questions = group.sample(n=min(4, len(group)), random_state=42)\n",
    "\n",
    "    for _, row in sampled_questions.iterrows():\n",
    "        extracted_keywords = extract_keywords(row[\"question\"])  # Extract keywords from LLM\n",
    "        keyword_list = extracted_keywords.strip(\"[]\").replace('\"', '').split(\", \")  \n",
    "\n",
    "        # Store formatted example\n",
    "        few_shot_examples.append((row[\"question\"], keyword_list))\n",
    "\n",
    "        # Collect all extracted keywords for LSH indexing\n",
    "        keyword_store.extend(keyword_list)\n",
    "\n",
    "# Initialize LSH index\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "index = {}\n",
    "\n",
    "# Add all extracted keywords to LSH\n",
    "for idx, word in enumerate(set(keyword_store)):  # Unique keywords only\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    minhash.update(word.encode(\"utf8\"))  \n",
    "    lsh.insert(str(idx), minhash)\n",
    "    index[str(idx)] = word  \n",
    "\n",
    "# Function to retrieve similar keywords\n",
    "def retrieve_similar_keywords(query, top_k=5):\n",
    "    query_minhash = MinHash(num_perm=128)\n",
    "    query_minhash.update(query.encode(\"utf8\"))\n",
    "\n",
    "    # Retrieve candidates from LSH\n",
    "    candidate_ids = lsh.query(query_minhash)\n",
    "\n",
    "    # Compute similarity\n",
    "    ranked_results = []\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0]\n",
    "\n",
    "    for idx in candidate_ids:\n",
    "        candidate_word = index[idx]\n",
    "        candidate_embedding = embedding_model.encode([candidate_word], convert_to_numpy=True)[0]\n",
    "\n",
    "        # Semantic similarity\n",
    "        semantic_sim = np.dot(query_embedding, candidate_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding))\n",
    "\n",
    "        # Typo distance\n",
    "        edit_dist = Levenshtein.distance(query, candidate_word)\n",
    "\n",
    "        ranked_results.append((candidate_word, semantic_sim, edit_dist))\n",
    "\n",
    "    # Sort: higher similarity + lower typo distance\n",
    "    ranked_results.sort(key=lambda x: (-x[1], x[2]))\n",
    "\n",
    "    return ranked_results[:top_k]\n",
    "\n",
    "# Save the few-shot examples\n",
    "with open(\"few_shot_prompt.txt\", \"w\") as f:\n",
    "    for question, keywords in few_shot_examples:\n",
    "        f.write(f'Question: \"{question}\"\\nKeywords: {keywords}\\n\\n')\n",
    "\n",
    "print(\"Few-shot examples saved. Here’s a preview:\\n\")\n",
    "for question, keywords in few_shot_examples[:8]:\n",
    "    print(f'Question: \"{question}\"\\nKeywords: {keywords}\\n')\n",
    "\n",
    "# Example retrieval\n",
    "test_word = \"faculty\"\n",
    "similar_words = retrieve_similar_keywords(test_word, top_k=5)\n",
    "print(f\"Top similar words to '{test_word}': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de2a7bc-283a-4a3f-8370-57049c71056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daniel\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5a0db-7a74-48c6-ad5d-055849cd9ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huggingface_env)",
   "language": "python",
   "name": "huggingface_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
