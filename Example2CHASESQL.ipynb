{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c408f133-3f66-4adf-85cc-89cc563d610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\") #See if your kernel crasher or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b6e46b3-92fa-43c3-b5d8-e82a63b29bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 11807.43359375 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def check_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Memory Usage: {process.memory_info().rss / (1024 * 1024)} MB\")\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07d64481-5dad-447c-8223-e4f4bd8fd7ac",
   "metadata": {},
   "source": [
    "I am checking the memory usage as the kernel is dying in the big code block. I am trying the packages individually to see if one of them is causing the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e97cd47-218f-4383-b8ca-41216caa4f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello, how are you? A little. But really, you must have done two things to find'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "test_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\", device=0)\n",
    "print(test_pipeline(\"Hello, how are you?\", max_length=20, num_return_sequences=1, truncation=True, pad_token_id=50256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "589883e8-ac82-49fd-8459-07037fa045ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['Extract', 'keywords', 'from', 'the', 'following', 'question:', 'Find', 'all', 'customers', 'who', 'bought', 'products', 'in', '2023.', 'CUSTOM', 'RATIC']\n",
      "Similar Words: ['customer', 'product', 'purchase', 'order', 'year', 'customer', 'purchase', 'order', 'year', 'order', 'year', 'customer', 'product', 'purchase', 'order', 'year', 'customer', 'product', 'purchase', 'order', 'year', 'product', 'purchase', 'year', 'customer', 'product', 'purchase', 'order', 'year', 'customer', 'product', 'purchase', 'customer', 'product', 'purchase', 'order', '2023']\n",
      "⚠️ WARNUNG: 'Extract' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'keywords' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'from' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'the' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'following' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'question:' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'Find' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'all' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'customers' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'who' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'bought' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'products' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'in' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: '2023.' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'CUSTOM' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "⚠️ WARNUNG: 'RATIC' nicht in word_embeddings vorhanden!\n",
      "Verfügbare Wörter: ['customer', 'product', '2023', 'purchase', 'order', 'year']\n",
      "Re-ranked Words: ['product', 'product', 'product', 'product', 'product', 'product', 'product', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', '2023', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'product', 'product', 'product', 'product', 'product', 'product', 'product', '2023', 'order', 'order', 'order', 'order', 'order', 'order', 'order', '2023', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'order', 'order', 'order', 'order', 'order', 'order', 'order', '2023', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', '2023', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', '2023', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'year', 'year', 'year', 'year', 'year', 'year', 'year', '2023', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', '2023', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', '2023', 'year', 'year', 'year', 'year', 'year', 'year', 'year', '2023', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'year', 'year', 'year', 'year', 'year', 'year', 'year', '2023', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', '2023', 'year', 'year', 'year', 'year', 'year', 'year', 'year', '2023', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', '2023', 'year', 'year', 'year', 'year', 'year', 'year', 'year', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'year', 'year', 'year', 'year', 'year', 'year', 'year', '2023', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'year', 'year', 'year', 'year', 'year', 'year', 'year', '2023', 'order', 'order', 'order', 'order', 'order', 'order', 'order', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'customer', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase', 'purchase']\n",
      "Generated SQL Query: SELECT * FROM customers WHERE \n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import pipeline\n",
    "from datasketch import MinHashLSHForest, MinHash\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Keyword Extraction\n",
    "def extract_keywords(question):\n",
    "    \"\"\"\n",
    "    Extract keywords from a natural language question using a smaller LLM.\n",
    "    \"\"\"\n",
    "    # Load a smaller LLM for keyword extraction\n",
    "    keyword_extractor = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    \n",
    "    # Generate keywords\n",
    "    prompt = f\"Extract keywords from the following question: {question}\"\n",
    "    keywords = keyword_extractor(prompt, max_length=50, num_return_sequences=1)\n",
    "    return keywords[0]['generated_text'].split()  # Simple split for demo purposes\n",
    "\n",
    "# Step 2: Locality-Sensitive Hashing (LSH)\n",
    "def create_lsh_forest(database_words):\n",
    "    \"\"\"\n",
    "    Create an LSH Forest and add database words to it.\n",
    "    \"\"\"\n",
    "    forest = MinHashLSHForest(num_perm=128)\n",
    "    \n",
    "    for i, word in enumerate(database_words):\n",
    "        m = MinHash(num_perm=128)\n",
    "        for char in word:\n",
    "            m.update(char.encode('utf-8'))\n",
    "        forest.add(i, m)\n",
    "    \n",
    "    # Index the forest\n",
    "    forest.index()\n",
    "    return forest\n",
    "\n",
    "def find_similar_words(query_word, forest, database_words, top_k=5):\n",
    "    \"\"\"\n",
    "    Find similar words in the database using LSH.\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=128)\n",
    "    for char in query_word:\n",
    "        m.update(char.encode('utf-8'))\n",
    "    results = forest.query(m, top_k)\n",
    "    return [database_words[i] for i in results]\n",
    "\n",
    "# Step 3: Re-ranking\n",
    "def re_rank_words(query_word, similar_words, word_embeddings):\n",
    "    \"\"\"\n",
    "    Re-rank similar words based on embedding similarity and edit distance.\n",
    "    \"\"\"\n",
    "    if query_word not in word_embeddings:  # Debugging: Check if the word is missing\n",
    "        print(f\"!!!⚠️ WARNUNG: '{query_word}' nicht in word_embeddings vorhanden!!!\")\n",
    "        print(f\"Verfügbare Wörter: {list(word_embeddings.keys())}\")\n",
    "    \n",
    "    query_embedding = word_embeddings.get(query_word, np.random.rand(32))  # Falls nicht vorhanden, erzeuge zufälliges Embedding\n",
    "    scores = []\n",
    "\n",
    "    for word in similar_words:\n",
    "        # Cosine similarity\n",
    "        cosine_sim = cosine_similarity([query_embedding], [word_embeddings.get(word, np.random.rand(32))])[0][0]\n",
    "        # Edit distance\n",
    "        edit_dist = levenshtein_distance(query_word, word)\n",
    "        # Combined score (higher is better)\n",
    "        combined_score = cosine_sim - (edit_dist / 10)  # Adjust weights as needed\n",
    "        scores.append((word, combined_score))\n",
    "\n",
    "    # Sort by combined score\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, _ in scores]\n",
    "\n",
    "\n",
    "# Step 4: SQL Query Generation\n",
    "def generate_sql(keywords, re_ranked_words):\n",
    "    \"\"\"\n",
    "    Generate an SQL query using extracted keywords and re-ranked words.\n",
    "    \"\"\"\n",
    "    table = \"customers\"  # Replace with your table name\n",
    "    columns = [\"customer_id\", \"product\", \"year\"]  # Replace with your columns\n",
    "    sql = f\"SELECT * FROM {table} WHERE \"\n",
    "    conditions = []\n",
    "    \n",
    "    for keyword, word in zip(keywords, re_ranked_words):\n",
    "        if keyword in columns:\n",
    "            conditions.append(f\"{keyword} = '{word}'\")\n",
    "    \n",
    "    sql += \" AND \".join(conditions)\n",
    "    return sql\n",
    "\n",
    "# Main Pipeline\n",
    "def text_to_sql_pipeline(question, database_words, word_embeddings):\n",
    "    \"\"\"\n",
    "    Full pipeline to convert a natural language question into an SQL query.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract keywords\n",
    "    keywords = extract_keywords(question)\n",
    "    print(\"Extracted Keywords:\", keywords)\n",
    "    \n",
    "    # Step 2: Create LSH Forest and find similar words\n",
    "    forest = create_lsh_forest(database_words)\n",
    "    similar_words = []\n",
    "    for keyword in keywords:\n",
    "        similar_words.extend(find_similar_words(keyword, forest, database_words))\n",
    "    print(\"Similar Words:\", similar_words)\n",
    "    \n",
    "    # Step 3: Re-rank words\n",
    "    re_ranked_words = []\n",
    "    for keyword in keywords:\n",
    "        re_ranked_words.extend(re_rank_words(keyword, similar_words, word_embeddings))\n",
    "    print(\"Re-ranked Words:\", re_ranked_words)\n",
    "    \n",
    "    # Step 4: Generate SQL query\n",
    "    sql_query = generate_sql(keywords, re_ranked_words)\n",
    "    return sql_query\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example database words (replace with your actual database words)\n",
    "    database_words = [\"customer\", \"product\", \"2023\", \"purchase\", \"order\", \"year\"]\n",
    "    \n",
    "    # Example word embeddings (replace with actual embeddings from your LLM)\n",
    "    word_embeddings = {\n",
    "        \"customer\": np.random.rand(32),\n",
    "        \"product\": np.random.rand(32),\n",
    "        \"2023\": np.random.rand(32),\n",
    "        \"purchase\": np.random.rand(32),\n",
    "        \"order\": np.random.rand(32),\n",
    "        \"year\": np.random.rand(32),\n",
    "    }\n",
    "    \n",
    "    # Example question\n",
    "    question = \"Find all customers who bought products in 2023.\"\n",
    "    \n",
    "    # Run the pipeline\n",
    "    sql_query = text_to_sql_pipeline(question, database_words, word_embeddings)\n",
    "    print(\"Generated SQL Query:\", sql_query)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58620679-8717-439f-ba18-4f22c8edd8ba",
   "metadata": {},
   "source": [
    "New try as previous code did not work, now llama 3.2 is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc488e1b-1572-42e9-beca-03338aee831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure ollama for python is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e400f37-b379-4dc3-a967-5497b8c12f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "#having an older torch audio like 2.5.1 should not affect much. Even thhough ollama recommend 2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221c4608-10dc-4a88-85b2-a3d31d9aa7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "423ba183-bc4f-43b0-ac00-282c09561f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: Here are the extracted keywords:\n",
      "\n",
      "1. Stocks\n",
      "2. Technology\n",
      "3. Sector\n",
      "4. 2024 (year)\n",
      "\n",
      "Let me know if you need any further assistance!\n"
     ]
    }
   ],
   "source": [
    "# Using Ollama to run Llama3.2 for keyword extraction as first try for the few shot examples\n",
    "def extract_keywords(text):\n",
    "    response = ollama.chat(\n",
    "        model='llama3.1',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Extract keywords from this text: '{text}'\"}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example input\n",
    "question = \"What are the top-performing stocks in the technology sector for 2024?\"\n",
    "keywords = extract_keywords(question)\n",
    "print(\"Extracted Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc4d93f0-8f24-4b60-8303-c353993cd3df",
   "metadata": {},
   "source": [
    "Wonderful, now Ollama 3.2 is capable of extraction keywords out of a sentence. Next step is to download the SPIDER dataset, we will do it by installing datasets and the using the function load_dataset(\"spider\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df97d27f-808e-45ae-85dc-4115cfb48ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce03dad9d0cd43129ef8f7844405dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['db_id', 'query', 'question', 'schema', 'query_res'],\n",
      "        num_rows: 7672\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['db_id', 'query', 'question', 'schema', 'query_res'],\n",
      "        num_rows: 665\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#SPIDER dataset from Hugging Face download\n",
    "dataset = load_dataset(\"CM/spider\")\n",
    "\n",
    "#preview\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5887c3f3-f81a-4a08-8fa2-694767a8a737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21277bf642f249aca7c6024b82d90566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow\n",
    "from datasets import load_dataset\n",
    "##I had to downgrade pyarrow to 12.01. as the version 19.0 had an error that prevented the from running. Still the prompt warned about datasets needing at least the pyarrow 1.5 version\n",
    "#Downgrading did not worked, then I noticed the dataset installation i had was old, and procced to update pyarrow and datasets\n",
    "ds = load_dataset(\"CM/spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a70050d-3e69-490a-90c7-bae4d13220f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['db_id', 'query', 'question', 'schema', 'query_res'],\n",
      "        num_rows: 7672\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['db_id', 'query', 'question', 'schema', 'query_res'],\n",
      "        num_rows: 665\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Okay this indicates that the data set was successfully downloaded\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b89feaa-25a2-41b3-a641-67feb3a3e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   db_id                                              query  \\\n",
      "0  department_management         SELECT count(*) FROM head WHERE age  >  56   \n",
      "1  department_management  SELECT name ,  born_state ,  age FROM head ORD...   \n",
      "2  department_management  SELECT creation ,  name ,  budget_in_billions ...   \n",
      "3  department_management  SELECT max(budget_in_billions) ,  min(budget_i...   \n",
      "4  department_management  SELECT avg(num_employees) FROM department WHER...   \n",
      "5  department_management  SELECT name FROM head WHERE born_state != 'Cal...   \n",
      "6  department_management  SELECT DISTINCT T1.creation FROM department AS...   \n",
      "7  department_management  SELECT born_state FROM head GROUP BY born_stat...   \n",
      "8  department_management  SELECT creation FROM department GROUP BY creat...   \n",
      "9  department_management  SELECT T1.name ,  T1.num_employees FROM depart...   \n",
      "\n",
      "                                            question  \\\n",
      "0  How many heads of the departments are older th...   \n",
      "1  List the name, born state and age of the heads...   \n",
      "2  List the creation year, name and budget of eac...   \n",
      "3  What are the maximum and minimum budget of the...   \n",
      "4  What is the average number of employees of the...   \n",
      "5  What are the names of the heads who are born o...   \n",
      "6  What are the distinct creation years of the de...   \n",
      "7  What are the names of the states where at leas...   \n",
      "8   In which year were most departments established?   \n",
      "9  Show the name and number of employees for the ...   \n",
      "\n",
      "                                              schema  \\\n",
      "0  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "1  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "2  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "3  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "4  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "5  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "6  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "7  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "8  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "9  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...   \n",
      "\n",
      "                                           query_res  \n",
      "0                                             (5,)\\n  \n",
      "1  ('Pádraig Harrington', 'Connecticut', 43.0)\\n(...  \n",
      "2  ('1789', 'State', 9.96)\\n('1789', 'Treasury', ...  \n",
      "3                                     (543.2, 6.2)\\n  \n",
      "4                            (105468.16666666667,)\\n  \n",
      "5  ('Tiger Woods',)\\n('K. J. Choi',)\\n('Jeff Magg...  \n",
      "6                                        ('1903',)\\n  \n",
      "7                                  ('California',)\\n  \n",
      "8                                        ('1789',)\\n  \n",
      "9  ('Treasury', 115897.0)\\n('Homeland Security', ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the training set to a pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(df_train.head(10))\n",
    "\n",
    "#As visualized in the huggingface dataset viewer, the dataset has 6 features(or columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b54b706-6fbd-4f07-bf18-aba22239efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"distinct creation years of the departments\"] \n",
      "\n",
      "However, if we follow the format you provided earlier, it seems like we're looking for individual words that can be used as search terms. In this case, the question is asking about \"years\" of the departments, so I would extract the following keywords:\n",
      "\n",
      "[\"distinct\", \"creation\", \"years\", \"departments\"]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "def extract_keywords(text):\n",
    "    few_shot_examples = \"\"\"Extract keywords from the following questions. Examples:\n",
    "    \n",
    "    Question: \"How many heads of the departments are older than 56?\"\n",
    "    Keywords: [\"heads\", \"departments\", \"older\", \"56\"]\n",
    "    \n",
    "    Question: \"List the name, born state, and age of the heads of departments ordered by age.\"\n",
    "    Keywords: [\"name\", \"born state\", \"age\", \"heads\", \"departments\", \"ordered\", \"age\"]\n",
    "    \n",
    "    Question: \"What is the average number of employees of the departments where budget is over 1 billion?\"\n",
    "    Keywords: [\"average\", \"number of employees\", \"departments\", \"budget\", \"over\", \"1 billion\"]\n",
    "    \n",
    "    Now extract keywords from this new question:\n",
    "    Question: \"{text}\"\n",
    "    Keywords:\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.1',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": few_shot_examples.format(text=text)}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Test with an example from the dataset\n",
    "question = \"What are the distinct creation years of the departments?\"\n",
    "keywords = extract_keywords(question)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aab25a8a-9c08-43d1-86ea-5ec7c1d8f72d",
   "metadata": {},
   "source": [
    "In the few shots examples I plan to add some examples of other categories of questions according to the different questions present in the spider dataset. First lets list them all using pandas functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "539c0cf3-2078-47bf-879c-e98ec21e08a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['department_management', 'farm', 'student_assessment', 'bike_1', 'book_2', 'musical', 'product_catalog', 'flight_1', 'allergy_1', 'store_1', 'journal_committee', 'customers_card_transactions', 'race_track', 'coffee_shop', 'insurance_fnol', 'medicine_enzyme_interaction', 'university_basketball', 'phone_1', 'match_season', 'climbing', 'body_builder', 'election_representative', 'apartment_rentals', 'game_injury', 'soccer_1', 'performance_attendance', 'debate', 'insurance_and_eClaims', 'customers_and_invoices', 'wedding', 'theme_gallery', 'riding_club', 'gymnast', 'browser_web', 'wrestler', 'school_finance', 'protein_institute', 'cinema', 'products_for_hire', 'phone_market', 'gas_company', 'party_people', 'pilot_record', 'cre_Doc_Control_Systems', 'local_govt_in_alabama', 'machine_repair', 'entrepreneur', 'perpetrator', 'csu_1', 'candidate_poll', 'movie_1', 'county_public_safety', 'local_govt_mdm', 'party_host', 'storm_record', 'election', 'news_report', 'restaurant_1', 'customer_deliveries', 'sakila_1', 'loan_1', 'behavior_monitoring', 'assets_maintenance', 'station_weather', 'sports_competition', 'manufacturer', 'hr_1', 'music_1', 'baseball_1', 'mountain_photos', 'program_share', 'e_learning', 'insurance_policies', 'hospital_1', 'ship_mission', 'company_employee', 'film_rank', 'cre_Doc_Tracking_DB', 'club_1', 'tracking_grants_for_research', 'network_2', 'decoration_competition', 'document_management', 'company_office', 'solvency_ii', 'entertainment_awards', 'customers_campaigns_ecommerce', 'college_3', 'department_store', 'aircraft', 'local_govt_and_lot', 'school_player', 'store_product', 'soccer_2', 'device', 'cre_Drama_Workshop_Groups', 'music_2', 'manufactory_1', 'tracking_software_problems', 'shop_membership', 'voter_2', 'products_gen_characteristics', 'swimming', 'railway', 'customers_and_products_contacts', 'dorm_1', 'customer_complaints', 'workshop_paper', 'tracking_share_transactions', 'cre_Theme_park', 'game_1', 'customers_and_addresses', 'music_4', 'roller_coaster', 'ship_1', 'city_record', 'e_government', 'school_bus', 'flight_company', 'cre_Docs_and_Epenses', 'scientist_1', 'train_station', 'driving_school', 'activity_1', 'tracking_orders', 'architecture', 'culture_company', 'geo', 'scholar', 'yelp', 'academic', 'imdb', 'restaurants']\n"
     ]
    }
   ],
   "source": [
    "unique_db_ids = ds[\"train\"].unique(\"db_id\")\n",
    "print(unique_db_ids)\n",
    "#this will list all unique entries in db_id column, remember that is necessary to either use the train or test split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af4833fa-2fa8-40c5-b290-15db30005b8a",
   "metadata": {},
   "source": [
    "As it can be seem there are many different themes, so in order to balance it, I will take 4 examples of each topic to have a \"balanced\" few shots. This lacks of any source, so it is more of a gut feeling. It might be worth looking into it further, if it makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c06dfcc0-f669-4e23-bcf9-e1e2dc45805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot examples saved. Here’s a preview:\n",
      "\n",
      "Question: \"return me the authors who have papers in VLDB conference before 2002 .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"authors\", \"papers\", \"VLDB\", \"conference\", \"before\", \"2002\"]\n",
      "\n",
      "Question: \"return me all the papers, which contain the keyword \" Natural Language \" .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"papers\", \"keyword\", \"Natural Language\"]\n",
      "\n",
      "Question: \"return me the author in the \" University of Michigan \" whose papers have the most total citations .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"author\", \"University of Michigan\", \"papers\", \"citations\"]\n",
      "\n",
      "Question: \"return me the number of papers on VLDB conference .\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"papers\", \"VLBL\", \"conference\"]\n",
      "\n",
      "Question: \"What is the first and last name of the student participating in the most activities?\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"first\", \"name\", \"last\", \"student\", \"participating\", \"activities\"]\n",
      "\n",
      "Question: \"How many faculty do we have?\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"faculty\"]\n",
      "\n",
      "Question: \"For each faculty rank, show the number of faculty members who have it.\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"each\", \"faculty rank\", \"number of faculty members\", \"who\", \"it\"]\n",
      "\n",
      "Question: \"Which building has most faculty members?\"\n",
      "Keywords: Here are the extracted keywords:\n",
      "\n",
      "[\"building\", \"faculty\", \"members\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "\n",
    "# Function to extract keywords using an LLM\n",
    "def extract_keywords(text):\n",
    "    few_shot_prompt = \"\"\"Extract keywords from the following questions. Examples:\n",
    "    \n",
    "    Question: \"How many heads of the departments are older than 56?\"\n",
    "    Keywords: [\"heads\", \"departments\", \"older\", \"56\"]\n",
    "    \n",
    "    Question: \"List the name, born state, and age of the heads of departments ordered by age.\"\n",
    "    Keywords: [\"name\", \"born state\", \"age\", \"heads\", \"departments\", \"ordered\", \"age\"]\n",
    "    \n",
    "    Question: \"What is the average number of employees of the departments where budget is over 1 billion?\"\n",
    "    Keywords: [\"average\", \"number of employees\", \"departments\", \"budget\", \"over\", \"1 billion\"]\n",
    "    \n",
    "    Now extract keywords from this new question:\n",
    "    Question: \"{text}\"\n",
    "    Keywords:\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.1',\n",
    "        messages=[{\"role\": \"user\", \"content\": few_shot_prompt.format(text=text)}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "\n",
    "# Group dataset by db_id\n",
    "grouped = df_train.groupby(\"db_id\")\n",
    "\n",
    "few_shot_examples = []\n",
    "\n",
    "# Loop through each topic\n",
    "for db_id, group in grouped:\n",
    "    sampled_questions = group.sample(n=min(4, len(group)), random_state=42)\n",
    "\n",
    "    for _, row in sampled_questions.iterrows():\n",
    "        # Extract keywords using LLM\n",
    "        extracted_keywords = extract_keywords(row[\"question\"])\n",
    "\n",
    "        # Store formatted example\n",
    "        few_shot_examples.append(\n",
    "            f'Question: \"{row[\"question\"]}\"\\nKeywords: {extracted_keywords}\\n'\n",
    "        )\n",
    "\n",
    "# Save the examples\n",
    "with open(\"few_shot_prompt.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(few_shot_examples))\n",
    "\n",
    "print(\"Few-shot examples saved. Here’s a preview:\\n\")\n",
    "print(\"\\n\".join(few_shot_examples[:8]))  # Show first 8 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fa8647b-f441-4b5d-9c00-b9adcda65b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_id\n",
      "academic             181\n",
      "activity_1            88\n",
      "aircraft              46\n",
      "allergy_1             98\n",
      "apartment_rentals     80\n",
      "                    ... \n",
      "voter_2               72\n",
      "wedding               20\n",
      "workshop_paper        30\n",
      "wrestler              40\n",
      "yelp                 111\n",
      "Name: question, Length: 133, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(grouped[\"question\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0069718f-5565-4071-9884-ca3c7795cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (2.6.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (0.29.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\larir\\miniconda3\\envs\\projekt\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70ac9897-98aa-4b59-a5d2-c427fb884ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Save the few-shot examples\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfew_shot_prompt.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question, keywords \u001b[38;5;129;01min\u001b[39;00m few_shot_examples:\n\u001b[0;32m     69\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKeywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFew-shot examples saved. Here’s a preview:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ollama\n",
    "import random\n",
    "import Levenshtein\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#list ot store the keywords\n",
    "keyword_store = []\n",
    "\n",
    "# Loop through each topic\n",
    "for db_id, group in grouped:\n",
    "    sampled_questions = group.sample(n=min(4, len(group)), random_state=42)\n",
    "\n",
    "    for _, row in sampled_questions.iterrows():\n",
    "        extracted_keywords = extract_keywords(row[\"question\"])  # Extract keywords from LLM\n",
    "        keyword_list = extracted_keywords.strip(\"[]\").replace('\"', '').split(\", \")  \n",
    "\n",
    "        # Store formatted example\n",
    "        few_shot_examples.append((row[\"question\"], keyword_list))\n",
    "\n",
    "        # Collect all extracted keywords for LSH indexing\n",
    "        keyword_store.extend(keyword_list)\n",
    "\n",
    "# Initialize LSH index\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "index = {}\n",
    "\n",
    "# Add all extracted keywords to LSH\n",
    "for idx, word in enumerate(set(keyword_store)):  # Unique keywords only\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    minhash.update(word.encode(\"utf8\"))  \n",
    "    lsh.insert(str(idx), minhash)\n",
    "    index[str(idx)] = word  \n",
    "\n",
    "# Function to retrieve similar keywords\n",
    "def retrieve_similar_keywords(query, top_k=5):\n",
    "    query_minhash = MinHash(num_perm=128)\n",
    "    query_minhash.update(query.encode(\"utf8\"))\n",
    "\n",
    "    # Retrieve candidates from LSH\n",
    "    candidate_ids = lsh.query(query_minhash)\n",
    "\n",
    "    # Compute similarity\n",
    "    ranked_results = []\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0]\n",
    "\n",
    "    for idx in candidate_ids:\n",
    "        candidate_word = index[idx]\n",
    "        candidate_embedding = embedding_model.encode([candidate_word], convert_to_numpy=True)[0]\n",
    "\n",
    "        # Semantic similarity\n",
    "        semantic_sim = np.dot(query_embedding, candidate_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding))\n",
    "\n",
    "        # Typo distance\n",
    "        edit_dist = Levenshtein.distance(query, candidate_word)\n",
    "\n",
    "        ranked_results.append((candidate_word, semantic_sim, edit_dist))\n",
    "\n",
    "    # Sort: higher similarity + lower typo distance\n",
    "    ranked_results.sort(key=lambda x: (-x[1], x[2]))\n",
    "\n",
    "    return ranked_results[:top_k]\n",
    "\n",
    "# Save the few-shot examples\n",
    "with open(\"few_shot_prompt.txt\", \"w\") as f:\n",
    "    for question, keywords in few_shot_examples:\n",
    "        f.write(f'Question: \"{question}\"\\nKeywords: {keywords}\\n\\n')\n",
    "\n",
    "print(\"Few-shot examples saved. Here’s a preview:\\n\")\n",
    "for question, keywords in few_shot_examples[:8]:\n",
    "    print(f'Question: \"{question}\"\\nKeywords: {keywords}\\n')\n",
    "\n",
    "# Example retrieval\n",
    "test_word = \"faculty\"\n",
    "similar_words = retrieve_similar_keywords(test_word, top_k=5)\n",
    "print(f\"Top similar words to '{test_word}': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5a0db-7a74-48c6-ad5d-055849cd9ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
