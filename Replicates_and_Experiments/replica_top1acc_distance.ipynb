{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f919bf-cd19-4ff1-82d3-c4fbf45dc928",
   "metadata": {},
   "source": [
    "# Replikation von Tabelle 13 und 14 aus dem Paper (TinyStories)\n",
    "\n",
    "## Ziel\n",
    "Berechnung von:\n",
    "- **Top-1 Accuracy** (Tabelle 13)\n",
    "- **Variational Distance** (Tabelle 14)  \n",
    "fÃ¼r ein eigenes Modell auf dem **TinyStories-Datensatz**.\n",
    "\n",
    "## Vorgehen\n",
    "\n",
    "### 1. Laden der Regel-Daten\n",
    "- Die Regel-Daten enthalten Kontexte und N-Gram-ZÃ¤hler (`next_token_counter`), die aus TinyStories extrahiert wurden.\n",
    "- Sie dienen als Referenz zur Bewertung des Modells.\n",
    "\n",
    "### 2. Modellvorhersagen berechnen\n",
    "- FÃ¼r jeden Kontext wird das Modell verwendet, um die **Wahrscheinlichkeitsverteilung fÃ¼r das nÃ¤chste Token** zu berechnen.\n",
    "- Daraus wird das **Top-1 Token des Modells** bestimmt.\n",
    "\n",
    "### 3. Vergleich mit Regeln\n",
    "- Die **Regel-ZÃ¤hler** werden in Wahrscheinlichkeiten umgerechnet.\n",
    "- Dann wird geprÃ¼ft, ob das Modell **das gleiche Top-1 Token** wie die Regel vorhersagt.\n",
    "- ZusÃ¤tzlich wird die **variationale Distanz** zwischen Modell und Regel berechnet.\n",
    "\n",
    "### 4. Gruppierung nach KontextlÃ¤nge\n",
    "- Die Ergebnisse werden nach der **KontextlÃ¤nge (1â€“7 Tokens)** gruppiert.\n",
    "- FÃ¼r jede LÃ¤nge werden die **durchschnittliche Top-1 Accuracy** und **Distanz** berechnet.\n",
    "\n",
    "### 5. Export im Paper-Stil\n",
    "- Die Ergebnisse werden in zwei Tabellen im Format des Papers gespeichert:\n",
    "    - **Zeilen:** ModellgrÃ¶ÃŸe (hier nur â€ž124Mâ€œ)\n",
    "    - **Spalten:** KontextlÃ¤ngen **1â€“7**\n",
    ".\n",
    "\n",
    "### 6. Speichern als CSV\n",
    "- Die Tabellen werden als **CSV-Dateien** gespeichert:\n",
    "    - `table_top1_accuracy.csv`\n",
    "    - `table_distance.csv`\n",
    "\n",
    "## Beispiel: Tabellenstruktur\n",
    "\n",
    "| Model Size / Context Length | 1   | 2   | 3   | 4   | 5   | 6   | 7   |\n",
    "|-----------------------------|-----|-----|-----|-----|-----|-----|-----|\n",
    "| 124M                        | x.x | x.x | x.x | x.x | x.x | x.x | x.x |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e122f8-0896-43ba-bdbd-36062be2a7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larir\\miniconda3\\envs\\projekt\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade 4 Regeln-Dateien fÃ¼r Tabelle 13, 14...\n",
      "Lade bis zu 4 Dateien von: gs://transformer-ngrams/TinyStories/eval_data_rules/\n",
      "Regeln-Daten geladen: (16000, 11)\n",
      "\n",
      "ðŸ“Š Top-1 Accuracy Tabelle:\n",
      "Context Length                   1      2      3      4      5      6      7\n",
      "Model Size / Context Length                                                 \n",
      "124M                         0.259  0.152  0.131  0.124  0.113  0.099  0.167\n",
      "\n",
      "ðŸ“Š Distanz Tabelle:\n",
      "Context Length                   1      2      3      4      5      6      7\n",
      "Model Size / Context Length                                                 \n",
      "124M                         0.633  0.767  0.834  0.863  0.876  0.897  0.904\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import gcsfs\n",
    "from transformers import AutoModelForCausalLM\n",
    "import glob\n",
    "\n",
    "# GPU aktivieren, falls verfÃ¼gbar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Google Cloud Storage verbinden\n",
    "fs = gcsfs.GCSFileSystem('transformer-ngrams')\n",
    "\n",
    "# Tokenizer laden\n",
    "TOKENIZER_PATH = 'gs://transformer-ngrams/32768.model'\n",
    "VOCAB_SIZE = 32768\n",
    "BOS_TOKEN = 1\n",
    "with fs.open(TOKENIZER_PATH) as f:\n",
    "    tokenizer = spm.SentencePieceProcessor(model_proto=f.read())\n",
    "\n",
    "# Transformer-Modell laden, unser trainiertes Modell\n",
    "model_name = \"dadosbon/TSModel2_124M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Funktion fÃ¼r Modellvorhersagen (angepasst fÃ¼r Geschwindigkeit)\n",
    "def get_model_predictions(input_tokens):\n",
    "    '''\n",
    "    Gibt die Wahrscheinlichkeitsverteilung fÃ¼r das nÃ¤chste Token zurÃ¼ck,\n",
    "    basierend auf dem gegebenen Kontext (input_tokens).\n",
    "\n",
    "    Parameter:\n",
    "        input_tokens (list of int): Liste von Token-IDs als Kontext.\n",
    "\n",
    "    RÃ¼ckgabe:\n",
    "        numpy.ndarray: Array der Wahrscheinlichkeiten (Softmax) Ã¼ber das Vokabular.\n",
    "    '''\n",
    "    input_tensor = torch.tensor([input_tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        logits = outputs.logits[:, -1, :VOCAB_SIZE]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "                # Debugging-Ausgabe\n",
    "        #print(\"Logits (min, max):\", logits.min().item(), logits.max().item())\n",
    "        #print(\"Erste 3 Logits:\", logits[0, :3])\n",
    "    return probs.cpu().numpy().flatten()\n",
    "\n",
    "def convert_counter_to_probs(arr):\n",
    "    '''\n",
    "    Wandelt ein N-Gram Counter-Array in eine Wahrscheinlichkeitsverteilung um.\n",
    "\n",
    "    Parameter:\n",
    "        arr (list or numpy.ndarray): Abwechselnd Token-ID und Count, z.B. [id1, count1, id2, count2, ...].\n",
    "        k (int): Anzahl der Top-k Tokens, die fÃ¼r Debugging extrahiert werden (optional, hier nicht verwendet).\n",
    "\n",
    "    RÃ¼ckgabe:\n",
    "        numpy.ndarray: Wahrscheinlichkeiten Ã¼ber das Vokabular, basierend auf N-Gram Counts.\n",
    "    '''\n",
    "    probs = np.zeros(VOCAB_SIZE)\n",
    "\n",
    "    if arr is None or len(arr) == 0:\n",
    "        return probs  # Falls leer, nur Nullen zurÃ¼ckgeben!\n",
    "\n",
    "    arr = np.array(arr).flatten()\n",
    "\n",
    "    # Falls die LÃ¤nge ungerade ist, entfernen wir das letzte Element\n",
    "    if len(arr) % 2 != 0:\n",
    "        arr = arr[:-1]\n",
    "\n",
    "    try:\n",
    "        tokens = arr[::2].astype(int)  # Token-IDs\n",
    "        counts = arr[1::2].astype(int)  # HÃ¤ufigkeiten\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Extrahieren von Tokens/Counts:\", e)\n",
    "        return probs  # Falls ein Fehler auftritt, bleibt es 0\n",
    "\n",
    "\n",
    "    if len(tokens) == 0 or len(counts) == 0:\n",
    "        return probs  # Falls etwas schiefgeht, alle Wahrscheinlichkeiten auf 0 setzen\n",
    "\n",
    "    total_count = np.sum(counts)\n",
    "    if total_count == 0:\n",
    "        return probs  # Falls alle Counts 0 sind, bleibt es 0!\n",
    "\n",
    "    token_probs = {int(token): count / total_count for token, count in zip(tokens, counts) if count > 0}\n",
    "\n",
    "    for token, prob in token_probs.items():\n",
    "        if 0 <= token < VOCAB_SIZE:\n",
    "            probs[token] = prob  # Wahrscheinlichkeiten setzen\n",
    "\n",
    "    return probs  # Falls leer, nur Nullen zurÃ¼ckgeben!\n",
    "\n",
    "    arr = np.array(arr).flatten()\n",
    "\n",
    "    # Falls die LÃ¤nge ungerade ist, entfernen wir das letzte Element\n",
    "    if len(arr) % 2 != 0:\n",
    "        arr = arr[:-1]\n",
    "\n",
    "    tokens = arr[::2]  # Token-IDs\n",
    "    counts = arr[1::2]  # HÃ¤ufigkeiten\n",
    "\n",
    "    if len(tokens) == 0 or len(counts) == 0:\n",
    "        return probs  # Falls etwas schiefgeht, alle Wahrscheinlichkeiten auf 0 setzen\n",
    "\n",
    "    total_count = np.sum(counts)\n",
    "    if total_count == 0:\n",
    "        return probs  # Falls alle Counts 0 sind, bleibt es 0!\n",
    "\n",
    "    token_probs = {int(token): count / total_count for token, count in zip(tokens, counts) if count > 0}\n",
    "    topk_tokens = sorted(token_probs, key=token_probs.get, reverse=True)[:k]\n",
    "\n",
    "    for token in topk_tokens:\n",
    "        if 0 <= token < VOCAB_SIZE:\n",
    "            probs[token] = token_probs[token]  # Wahrscheinlichkeiten setzen\n",
    "\n",
    "    return probs  # Falls leer, nur Nullen zurÃ¼ckgeben!\n",
    "\n",
    "    arr = np.array(arr).flatten()\n",
    "\n",
    "    # Falls die LÃ¤nge ungerade ist, entfernen wir das letzte Element\n",
    "    if len(arr) % 2 != 0:\n",
    "        arr = arr[:-1]\n",
    "\n",
    "    tokens = arr[::2]  # Token-IDs\n",
    "    counts = arr[1::2]  # HÃ¤ufigkeiten\n",
    "\n",
    "    total_count = np.sum(counts)\n",
    "    if total_count == 0:\n",
    "        return probs  # Falls alle Counts 0 sind, bleibt es 0!\n",
    "\n",
    "    token_probs = {int(token): count / total_count for token, count in zip(tokens, counts)}\n",
    "    topk_tokens = sorted(token_probs, key=token_probs.get, reverse=True)[:k]\n",
    "\n",
    "    for token in topk_tokens:\n",
    "        if 0 <= token < VOCAB_SIZE:\n",
    "            probs[token] = token_probs[token]  # Wahrscheinlichkeiten setzen\n",
    "\n",
    "    return probs\n",
    "    arr = np.array(arr).flatten()\n",
    "    if arr.ndim != 1 or len(arr) % 2 != 0:\n",
    "        return probs\n",
    "    tokens = arr[::2]\n",
    "    counts = arr[1::2]\n",
    "    total_count = np.sum(counts)\n",
    "    if total_count == 0:\n",
    "        return probs\n",
    "    token_probs = {int(token): count / total_count for token, count in zip(tokens, counts)}\n",
    "    topk_tokens = sorted(token_probs, key=token_probs.get, reverse=True)[:k]\n",
    "    for token in topk_tokens:\n",
    "        if token < VOCAB_SIZE:\n",
    "            probs[token] = token_probs[token]\n",
    "    return probs\n",
    "\n",
    "def dist(counter, model_probs):\n",
    "    '''\n",
    "    Berechnet die Variationale Distanz zwischen Regel-Prediktion und Modellvorhersage.\n",
    "\n",
    "    Parameter:\n",
    "        counter (list or numpy.ndarray): N-Gram Counter fÃ¼r die Regel (siehe oben).\n",
    "        model_probs (numpy.ndarray): Wahrscheinlichkeitsverteilung vom Modell.\n",
    "\n",
    "    RÃ¼ckgabe:\n",
    "        float: Variationale Distanz (0 bis 1) zwischen Regel und Modell.\n",
    "    '''\n",
    "    probs = convert_counter_to_probs(counter)\n",
    "    return 0.5 * np.sum(np.abs(probs - model_probs[:VOCAB_SIZE]))\n",
    "\n",
    "# Regeln-Daten laden AUSSCHNITT\n",
    "# Anzahl der zu ladenden Dateien\n",
    "num_files_to_load = 4  \n",
    "\n",
    "print(f\"Lade {num_files_to_load} Regeln-Dateien fÃ¼r Tabelle 13, 14...\")\n",
    "\n",
    "# Alle Pfade sortieren und eine Auswahl treffen\n",
    "parquet_paths = sorted(glob.glob('gs://transformer-ngrams/TinyStories/eval_data_rules/*.parquet'))[:num_files_to_load]\n",
    "\n",
    "def load_rules_with_model_predictions(\n",
    "    path_prefix=\"gs://transformer-ngrams/TinyStories/eval_data_rules/\",\n",
    "    max_files=4,\n",
    "    max_rows=5000,\n",
    "    sample_fraction=1.0\n",
    "):\n",
    "    '''\n",
    "    LÃ¤dt eine Stichprobe der Regel-Dateien und berechnet Modellvorhersagen.\n",
    "\n",
    "    Parameter:\n",
    "        path_prefix (str): Pfad-PrÃ¤fix zu den Parquet-Dateien.\n",
    "        max_files (int): Maximale Anzahl der zu ladenden Dateien.\n",
    "        max_rows (int): Maximale Zeilen pro Datei.\n",
    "        sample_fraction (float): Bruchteil der Zeilen, die nach dem max_rows-Sampling gezogen werden.\n",
    "\n",
    "    RÃ¼ckgabe:\n",
    "        pd.DataFrame: DataFrame mit Regeln und Modellvorhersagen.\n",
    "    '''\n",
    "\n",
    "    print(f\"Lade bis zu {max_files} Dateien von: {path_prefix}\")\n",
    "    parquet_paths = sorted(fs.ls(path_prefix))[:max_files]\n",
    "\n",
    "\n",
    "    dfs = []\n",
    "    for path in parquet_paths:\n",
    "        with fs.open(path, 'rb') as f:\n",
    "            df = pd.read_parquet(f)\n",
    "\n",
    "            # Optional Zeilenanzahl begrenzen\n",
    "            if max_rows is not None and len(df) > max_rows:\n",
    "                df = df.sample(n=max_rows, random_state=42)\n",
    "\n",
    "            # Optionalen Anteil auswÃ¤hlen\n",
    "            if sample_fraction < 1.0:\n",
    "                df = df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "    df_rules = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Regeln-Daten geladen:\", df_rules.shape)\n",
    "\n",
    "    # Modellvorhersagen berechnen\n",
    "    model_probs = []\n",
    "    for token_list in df_rules[\"token\"]:\n",
    "        probs = get_model_predictions([token_list])\n",
    "        model_probs.append(probs.tolist())\n",
    "\n",
    "    df_rules[\"model_probs\"] = model_probs\n",
    "    return df_rules\n",
    "\n",
    "df_rules_sample = load_rules_with_model_predictions(\n",
    "    max_files=4,         # wie viele Dateien maximal geladen werden sollen\n",
    "    max_rows=5000,       # maximal so viele Zeilen pro Datei\n",
    "    sample_fraction=0.80  # 1.0 = 100 %, 0.2 = nur 20 % jeder Datei\n",
    ")\n",
    "\n",
    "\n",
    "'''#FÃœR ALLE DATEN DANN:                    #Die beiden vorhergehenden BlÃ¶cke mit dem ersetzen, fÃ¼r ALLE daten\n",
    "# Alle Regeln-Daten laden\n",
    "print(\"Lade alle Regeln-Daten fÃ¼r Tabelle 13 & 14...\")\n",
    "parquet_files = fs.ls('transformer-ngrams/TinyStories/eval_data_rules/')\n",
    "df_list = []\n",
    "\n",
    "for path in parquet_files:\n",
    "    with fs.open(f'gs://{path}', 'rb') as f:\n",
    "        df_list.append(pd.read_parquet(f))\n",
    "\n",
    "df_rules = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Alle Regeln-Daten geladen:\", df_rules.shape)\n",
    "df_rules_sample = df_rules.copy()\n",
    "model_probs = []\n",
    "for token_list in df_rules_sample[\"token\"]:\n",
    "    probs = get_model_predictions([token_list])\n",
    "    model_probs.append(probs.tolist())'''\n",
    "\n",
    "\n",
    "\n",
    "#df_rules_sample[\"model_probs\"] = model_probs\n",
    "\n",
    "df_rules_sample[\"distance\"] = df_rules_sample.apply(\n",
    "    lambda row: dist(row[\"next_token_counter\"], row[\"model_probs\"]), axis=1\n",
    ")\n",
    "\n",
    "df_rules_sample[\"rule_prediction\"] = df_rules_sample[\"next_token_counter\"].apply(convert_counter_to_probs)\n",
    "df_rules_sample[\"model_top_1\"] = df_rules_sample[\"model_probs\"].apply(np.argmax)\n",
    "df_rules_sample[\"matches_rule\"] = df_rules_sample.apply(\n",
    "    lambda x: x[\"model_top_1\"] in np.argsort(x[\"rule_prediction\"])[-1:], axis=1\n",
    ")\n",
    "\n",
    "# Gruppierung nach KontextlÃ¤nge fÃ¼r Tabellen 13 & 14\n",
    "df_rules_sample[\"context_length\"] = df_rules_sample[\"context\"].apply(\n",
    "    lambda x: len([t for t in x.tolist() if t != 0]) if isinstance(x, np.ndarray) else 1\n",
    ")\n",
    "table_results = df_rules_sample.groupby(\"context_length\").agg(\n",
    "    top1_accuracy=(\"matches_rule\", \"mean\"),\n",
    "    avg_distance=(\"distance\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "table_results.rename(columns={\"context_length\": \"Regeln / KontextlÃ¤nge\", \n",
    "                               \"top1_accuracy\": \"Top-1 Genauigkeit\", \n",
    "                               \"avg_distance\": \"Variationale Distanz\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Modellname festlegen\n",
    "model_name = \"124M\"\n",
    "\n",
    "\n",
    "agg = df_rules_sample.groupby(\"context_length\").agg(\n",
    "    top1_accuracy=(\"matches_rule\", \"mean\"),\n",
    "    avg_distance=(\"distance\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Tabellen vorbereiten\n",
    "# â†’ Zeile: Modellname | Spalten: KontextlÃ¤ngen 1â€“7\n",
    "\n",
    "# Top-1 Accuracy Tabelle\n",
    "accuracy_row = agg.set_index(\"context_length\")[\"top1_accuracy\"]\n",
    "accuracy_table = pd.DataFrame([accuracy_row], index=[model_name])\n",
    "accuracy_table.columns.name = \"Context Length\"\n",
    "\n",
    "# Distanz Tabelle\n",
    "distance_row = agg.set_index(\"context_length\")[\"avg_distance\"]\n",
    "distance_table = pd.DataFrame([distance_row], index=[model_name])\n",
    "distance_table.columns.name = \"Context Length\"\n",
    "\n",
    "# Zeilenindex benennen â€“ das ist die linke obere Zelle!\n",
    "accuracy_table.index.name = \"Model Size / Context Length\"\n",
    "distance_table.index.name = \"Model Size / Context Length\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ausgabe prÃ¼fen\n",
    "print(\"\\nðŸ“Š Top-1 Accuracy Tabelle:\")\n",
    "print(accuracy_table.round(3))\n",
    "\n",
    "print(\"\\nðŸ“Š Distanz Tabelle:\")\n",
    "print(distance_table.round(3))\n",
    "\n",
    "# Optional: Als CSV speichern\n",
    "accuracy_table.round(3).to_csv(\"table_top1_accuracy.csv\")\n",
    "distance_table.round(3).to_csv(\"table_distance.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "table_results.to_csv(\"tiny_stories_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861bef0-709e-46ba-81ba-360676a8e156",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fba111-f96c-4ff2-8838-11195b0d12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige 5 Beispiele mit Modell vs. Regel | wie gut performt das Modell?\n",
    "for idx, row in df_rules_sample.head(5).iterrows():\n",
    "    context_tokens = row[\"token\"]\n",
    "    model_probs = row[\"model_probs\"]\n",
    "    rule_probs = row[\"rule_prediction\"]\n",
    "\n",
    "    model_top1 = np.argmax(model_probs)\n",
    "    rule_top1 = np.argmax(rule_probs)\n",
    "\n",
    "    print(f\"\\n Beispiel {idx}\")\n",
    "    print(\"Kontext (Token-IDs):\", context_tokens)\n",
    "    print(\"Kontext (Text):\", tokenizer.decode(context_tokens))\n",
    "    print(\"Model Top-1 ID:\", model_top1, \"| Token:\", tokenizer.id_to_piece(int(model_top1)))\n",
    "    print(\"Rule Top-1 ID:\", rule_top1, \"| Token:\", tokenizer.id_to_piece(int(rule_top1)))\n",
    "\n",
    "    print(\"Match?:\", model_top1 == rule_top1)\n",
    "\n",
    "    # Optional: Wahrscheinlichkeiten\n",
    "    print(\"Model Top-1 Wahrscheinlichkeit:\", model_probs[model_top1])\n",
    "    print(\"Rule Top-1 Wahrscheinlichkeit:\", rule_probs[rule_top1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cd99d-11de-42fa-8630-37f5a8bec695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
