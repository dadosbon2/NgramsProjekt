# Reproducing and Extending "Understanding Transformers via N-gram Statistics"
This repository contains code and analysis for our project based on the paper
"Understanding Transformers via N-gram Statistics" by Timothy Nguyen
(arXiv:2407.12034).

# Project Description
As part of our Data Mining course, we reproduce selected experiments from the paper and extend them with our own investigations. Our work includes:

- Reproducing key figures (e.g. Figure 5: N-gram matching vs model predictions)

- Implementing rule complexity analyses (Ïƒ-complexity)

- Designing new experiments to explore linguistic properties of N-gram rules

- Writing a final report summarizing our results

# Technologies
* Python 3.10+

* PyTorch

* Transformers (Hugging Face)

* NumPy, pandas

* Matplotlib, Seaborn

# Usage
Install the dependencies:

**pip install -r requirements.txt**
Then run one of the experiment scripts \\
The scripts generate outputs such as heatmaps, accuracy plots, and token-level comparisons.

# Structure
- to be continued.....

# Contributors
* Daniel Felipe Rivera-Cerquera
* Larissa Roth

# Reference
**Nguyen, Timothy. Understanding Transformers via N-gram Statistics. arXiv preprint arXiv:2407.12034, 2024.
https://doi.org/10.48550/arXiv.2407.12034**
