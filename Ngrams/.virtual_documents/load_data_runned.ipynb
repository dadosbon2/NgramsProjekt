


!pip install gcsfs
!pip install sentencepiece


from typing import Sequence

from collections import defaultdict
import itertools
import numpy as np
import pandas as pd

import gcsfs
import sentencepiece as spm

fs = gcsfs.GCSFileSystem('transformer-ngrams')

TOKENIZER_PATH = 'gs://transformer-ngrams/32768.model'
VOCAB_SIZE = 32768
BOS_TOKEN = 1


fs.ls('gs://transformer-ngrams')





fs.ls('gs://transformer-ngrams/TinyStories')








fs.ls('gs://transformer-ngrams/Wikipedia')





with fs.open(TOKENIZER_PATH) as f:
  tokenizer = spm.SentencePieceProcessor(model_proto=f.read())

# encode: text => id
print(tokenizer.encode_as_pieces('This is a test'))
print(tokenizer.encode_as_ids('This is a test'))

# decode: id => text
print(tokenizer.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))
print(tokenizer.decode_ids([209, 31, 9, 375, 586]))





MAX_CONTEXT_SIZE = 7

RULE_TYPES = {0: [()]}
RULE_TYPES.update({i+1: [('+',) + x for x in itertools.product(['-', '*', '+'], repeat=i)] for i in range(MAX_CONTEXT_SIZE)})

def get_rules_index(max_context_size):
  rules = {i: rule_list for i, rule_list in RULE_TYPES.items() if i <= max_context_size}
  return {i: r for i, r in enumerate(sum(rules.values(), []))}

RULES_INDEX = get_rules_index(MAX_CONTEXT_SIZE)
RULE_TO_INDEX = {v: k for k, v in RULES_INDEX.items()}
# 1094 (note: there are redundancies)
NUM_RULES = len(RULES_INDEX)

RULES_INDEX


RULES_SUFFIX = {i: rule for i, rule in RULES_INDEX.items() if all([r == '+' for r in rule])}
RULES_SUFFIX.update({0: ()})

# Rules which only use the suffix
RULES_SUFFIX


RULES_SUBGRAM = {i: rule for i, rule in RULES_INDEX.items() if all([r in ['-','+'] for r in rule])}
RULES_SUBGRAM.update({0: ()})

# Rules which only keep or drop tokens
RULES_SUBGRAM


def _get_possible_modified_contexts_for_rules(base_context: list[int]):
  """
  Given (t1, ..., tn) each token can be kept (+), marginalized (*), or dropped (-).
  Returns unique contexts from all possible such operations (which start with a +) and corresponding rule from RULE_INDEX
  """
  ret = []
  n = len(base_context)
  context_to_rule = defaultdict(list)
  for i in range(n): # create modified contexts where the ith token is untouched
    choices = itertools.product(['-', '*', '+'], repeat=n-i-1) # '-': drop token, '*': marginalize, '+': keep
    remaining_context = base_context[i+1:]
    for choice in choices:
      # prefix is always a '+'
      context = [base_context[i]]
      assert len(choice) == len(remaining_context)
      for c, token in zip(choice,remaining_context):
        if c == '-':
          continue
        elif c == '*':
          context.append(0)
        elif c == '+':
          context.append(token)
      context = tuple(context)
      ret.append(context)
      context_to_rule[context].append(RULE_TO_INDEX[('+',) + choice])
  unique_contexts = sorted(list(set(ret)), key=lambda x: (len(x), x), reverse=True)
  return unique_contexts, context_to_rule

def _get_contexts_and_rules(tokens, max_context_size) -> tuple[list[list[tuple[int]]], list[list[int]]]:
  """
  Goes through tokens and returns all possible modified contexts up to length max_len
  Also filter out any contexts in which 1 is not initial.
  """
  contexts_per_token = []
  context_to_rules_per_token = []
  for current_idx, t in enumerate(tokens[1:], 1):
    min_idx = max(0, current_idx-max_context_size)
    raw_context = list(tokens[min_idx:current_idx])
    unique_contexts, context_to_rule = _get_possible_modified_contexts_for_rules(raw_context,)
    contexts_per_token.append(unique_contexts)
    context_to_rules_per_token.append(context_to_rule)
  filter_fn = lambda x: (len(x) == 1) or (BOS_TOKEN not in x[1:])
  contexts_per_token = [list(filter(filter_fn, ctxs)) for ctxs in contexts_per_token]
  return contexts_per_token, context_to_rules_per_token

def get_df_ctx_rules_data(tokens: Sequence[int], max_context_size: int):
  """
  tokens: A sequence of nonzero integers. We assume BOS_TOKEN = 1.
  max_context_size: The maximum context to consider for rules. Assumed at most 7 given our RULES_INDEX has this bound.

  Returns a dataframe with 3 columns:
    index: the index of the current position
    context: a rule context (a tuple of ints) obtained from keeping, dropping, or marginalizing tokens; a 0 token corresponds to a marginalized token
    rule_index: the list of integers, corresponding to rules yielding that context (as obtained from RULES_INDEX)
  """
  assert max_context_size <= MAX_CONTEXT_SIZE
  contexts_data = []
  rules_data = []
  index_data = []
  ctxs_per_token, ctx_to_rule_per_token = _get_contexts_and_rules(tokens, max_context_size)
  for i, (current_ctxs, current_ctx_to_rules) in enumerate(zip(ctxs_per_token, ctx_to_rule_per_token)):
    contexts_data.extend(current_ctxs)
    index_data.extend([i for _ in range(len(current_ctxs))])
    for ctx in current_ctxs:
      rules_data.append(current_ctx_to_rules[ctx])
  df = pd.DataFrame({'index': index_data, 'context': contexts_data, 'rules': rules_data})
  df = df.sort_values(by=['index', 'rules'], key=lambda col: col if col.name == 'index' else col.apply(lambda x: x[0])).reset_index(drop=True)
  return df





get_df_ctx_rules_data([2,3,4,6,1,7], max_context_size=3)








SAMPLE_RULES_PATH = 'gs://transformer-ngrams/TinyStories/eval_data_rules/001.parquet'

with fs.open(SAMPLE_RULES_PATH, 'rb') as f:
  df_rules = pd.read_parquet(f)





df_rules


def convert_counter_to_probs(counter: list[int]):
  "counter = [k1 v1 ...] a sequence of key values, key = next token, value = count of next token"
  assert len(counter) % 2 == 0
  probs = np.zeros(VOCAB_SIZE)
  ks = counter[::2]
  assert BOS_TOKEN not in ks
  vs = counter[1::2]
  mass = sum(vs)
  for k, v in zip(ks, vs):
    probs[k] = v
  probs = probs / mass
  return probs

def dist(counter: Sequence[int], model_probs: np.ndarray):
  """
  Computes variational distance between the probability distribution from the counter and the given model_probs.
  """
  probs = convert_counter_to_probs(counter)
  return 0.5 * np.sum(np.abs(probs - model_probs))





n = len(set(df_rules['index']))
index = list(range(n))

random_probs = np.random.uniform(size=(n, VOCAB_SIZE))
random_probs = random_probs/np.sum(random_probs, axis=1)[:,None]

# Replace df_model_preds['model_probs'] with your model evaluated on the set of tokens given by df_rules['token'].
# The nth index should correspond to the predictive probability distribution of the mode evaluated on the sequence of tokens corresponding to index 0, ..., n-1 from df_rules.
df_model_preds = pd.DataFrame({'index': list(range(n)), 'model_probs': random_probs.tolist()})


df_joined = df_rules.merge(df_model_preds, on='index')
df_joined['distance'] = df_joined.apply(lambda x: dist(x.next_token_counter, x.model_probs), axis=1)


# Computes optimal rule and the associated top_1_acc per token
df_optimal = df_joined.loc[df_joined.groupby('index')['distance'].idxmin()].reset_index(drop=True)
df_optimal['top_1_acc'] = df_optimal.apply(lambda x: (np.argmax(x['model_probs']) in x['rule_prediction']) / len(x['rule_prediction']), axis=1)








TINYSTORES_TRAINING_DATA_PATH = (
    'gs://transformer-ngrams/TinyStories/training_data/'
)
WIKIPEDIA_TRAINING_DATA_PATH = (
    'gs://transformer-ngrams/Wikipedia/training_data/'
)
WIKIPEDIA_EVAL_DATA_PATH = 'gs://transformer-ngrams/Wikipedia/eval_data/'


with fs.open('gs://transformer-ngrams/TinyStories/training_data/001.parquet', 'rb') as f:
  df = pd.read_parquet(f)

sample_text = tokenizer.decode_ids(df.tokens.iloc[0].tolist())


df


print(fs.ls('gs://transformer-ngrams/'))



print(fs.ls('gs://transformer-ngrams/TinyStories'))




