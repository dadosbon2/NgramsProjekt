{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81ac3da-3ea9-4c5c-83cd-2a31ed758e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#später wieder einfügen für ALLE daten, jetzt nur ein paar Beispieldaten um zu sehen ob es klappt statt load sample\n",
    "\n",
    "# Lade die offiziellen N-Gramm-Statistiken der Autoren aus Google Cloud Storage\n",
    "def load_official_ngram_data():\n",
    "    file_list = fs.ls(GCS_BUCKET)\n",
    "    df_list = []\n",
    "    for file in file_list:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            with fs.open(file) as f:\n",
    "                df = pd.read_parquet(f)\n",
    "                df_list.append(df)\n",
    "    official_ngram_data = pd.concat(df_list, ignore_index=True)\n",
    "    return official_ngram_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3847349-487c-4dca-bad9-54bb0c985b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram Analysis Notebook\n",
    "\n",
    "# First, let's import all necessary libraries\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "from tqdm.notebook import tqdm  # For progress bars in Jupyter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# We'll create a mock gcsfs since you might not have access to the actual Google Cloud Storage\n",
    "class MockGCSFileSystem:\n",
    "    def __init__(self, project=None):\n",
    "        self.files = {}\n",
    "    \n",
    "    def ls(self, path):\n",
    "        # Return a list of mock file paths\n",
    "        return [f\"{path}/file_{i}.parquet\" for i in range(5)]\n",
    "    \n",
    "    def open(self, file_path):\n",
    "        # Create a dummy file-like object\n",
    "        import io\n",
    "        return io.BytesIO()\n",
    "\n",
    "# Try to import gcsfs, but use our mock if it fails\n",
    "try:\n",
    "    import gcsfs\n",
    "    fs = gcsfs.GCSFileSystem(project='transformer-ngrams')\n",
    "    GCS_BUCKET = 'gs://transformer-ngrams/TinyStories/train_data_rules'\n",
    "except ImportError:\n",
    "    print(\"Using mock GCS filesystem - no actual GCS data will be loaded\")\n",
    "    fs = MockGCSFileSystem()\n",
    "    GCS_BUCKET = 'mock-bucket/TinyStories/train_data_rules'\n",
    "\n",
    "# Configuration parameters\n",
    "MAX_FILES = 5\n",
    "MAX_ROWS = 5000\n",
    "SAMPLE_FRACTION = 0.15\n",
    "\n",
    "# Function to generate synthetic data for testing\n",
    "def generate_synthetic_data(num_rows=1000):\n",
    "    \"\"\"Generate synthetic data similar to what we'd expect from the GCS bucket\"\"\"\n",
    "    # Sample vocabulary for generating text\n",
    "    vocab = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \n",
    "             \"a\", \"man\", \"woman\", \"child\", \"house\", \"car\", \"tree\", \"sky\"]\n",
    "    \n",
    "    # Generate random text\n",
    "    texts = []\n",
    "    for _ in range(num_rows):\n",
    "        text_length = random.randint(10, 30)\n",
    "        text = \" \".join(random.choices(vocab, k=text_length))\n",
    "        texts.append(text)\n",
    "    \n",
    "    # Generate next token counters (simplified)\n",
    "    next_token_counters = []\n",
    "    for _ in range(num_rows):\n",
    "        # Create a list of random counts for potential next tokens\n",
    "        counter = [random.randint(1, 100) for _ in range(random.randint(3, 10))]\n",
    "        next_token_counters.append(counter)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'next_token_counter': next_token_counters,\n",
    "        'context_size_used': [random.randint(1, 7) for _ in range(num_rows)],\n",
    "        'target': [random.randint(1, 1000) for _ in range(num_rows)]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract n-grams from token lists\n",
    "def extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Extract n-grams from a list of tokens\"\"\"\n",
    "    return [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "# Chunk dataset into overlapping chunks\n",
    "def chunk_dataset(corpus: List[str], chunk_size: int = 2048, step: int = None) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Split tokens into overlapping chunks. If corpus is smaller than chunk_size,\n",
    "    the entire corpus is returned as a single chunk.\n",
    "    \"\"\"\n",
    "    if step is None:\n",
    "        step = chunk_size // 4  # 25% overlap for efficiency\n",
    "    \n",
    "    chunk_size = min(chunk_size, len(corpus))\n",
    "    if len(corpus) <= chunk_size:\n",
    "        return [corpus]\n",
    "    \n",
    "    return [corpus[i:i + chunk_size] for i in range(0, len(corpus) - chunk_size + 1, step)]\n",
    "\n",
    "# Compute n-gram statistics\n",
    "def compute_ngram_statistics(corpus_chunks: List[List[str]], max_n: int = 7):\n",
    "    \"\"\"Compute n-gram statistics for different values of n\"\"\"\n",
    "    ngram_counts = collections.defaultdict(collections.Counter)\n",
    "    \n",
    "    print(\"Computing n-gram statistics...\")\n",
    "    for chunk_idx, chunk in enumerate(tqdm(corpus_chunks, desc=\"Processing chunks\")):\n",
    "        # Show debug output for every 10th chunk up to 30 chunks\n",
    "        show_debug = (chunk_idx % 10 == 0 and chunk_idx < 30)\n",
    "        \n",
    "        for n in range(1, max_n + 1):\n",
    "            ngrams = extract_ngrams(chunk, n)\n",
    "            \n",
    "            if show_debug and n == 7 and ngrams:\n",
    "                print(f\"🔍 Example {n}-grams from chunk {chunk_idx}:\", ngrams[:3])\n",
    "                \n",
    "            for ngram in ngrams:\n",
    "                ngram_counts[n][ngram] += 1\n",
    "    \n",
    "    # Display the top n-grams for each value of n\n",
    "    for n in range(1, max_n + 1):\n",
    "        most_common = ngram_counts[n].most_common(5)\n",
    "        if most_common:\n",
    "            print(f\"📊 Top-5 {n}-grams:\", most_common)\n",
    "            print(f\"📊 Number of unique {n}-grams:\", len(ngram_counts[n]))\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "# Store n-gram statistics in JSON\n",
    "def store_ngram_statistics_json(ngram_counts, filename=\"ngrams.json\"):\n",
    "    \"\"\"Store n-gram statistics in a JSON file\"\"\"\n",
    "    # Convert only the most common n-grams to JSON to save space\n",
    "    max_ngrams_per_n = 10000\n",
    "    \n",
    "    json_data = {}\n",
    "    for n, counts in ngram_counts.items():\n",
    "        top_counts = dict(counts.most_common(max_ngrams_per_n))\n",
    "        json_data[str(n)] = {\" \".join(ngram): count for ngram, count in top_counts.items()}\n",
    "    \n",
    "    print(f\"Saving n-gram statistics to {filename}...\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f)\n",
    "    print(\"✅ Saved!\")\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "# Load a small sample of data\n",
    "def load_sample_data(use_synthetic=True, max_files=MAX_FILES, max_rows=MAX_ROWS, sample_fraction=SAMPLE_FRACTION):\n",
    "    \"\"\"Load a sample of data, either from GCS or synthetic\"\"\"\n",
    "    if use_synthetic:\n",
    "        print(\"Generating synthetic data for testing...\")\n",
    "        return generate_synthetic_data(num_rows=max_rows)\n",
    "    \n",
    "    file_list = fs.ls(GCS_BUCKET)\n",
    "    \n",
    "    # Take a stratified sample of files\n",
    "    if len(file_list) > max_files:\n",
    "        step = len(file_list) // max_files\n",
    "        file_list = [file_list[i] for i in range(0, len(file_list), step)][:max_files]\n",
    "    \n",
    "    print(f\"Loading data from {len(file_list)} files...\")\n",
    "    df_list = []\n",
    "    \n",
    "    for file_idx, file in enumerate(file_list):\n",
    "        if file.endswith(\".parquet\"):\n",
    "            print(f\"Processing file {file_idx+1}/{len(file_list)}: {file}\")\n",
    "            try:\n",
    "                with fs.open(file) as f:\n",
    "                    # Read only required columns\n",
    "                    try:\n",
    "                        df = pd.read_parquet(f, engine=\"pyarrow\", columns=[\"text\", \"next_token_counter\", \"context_size_used\"])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading specific columns: {e}\")\n",
    "                        print(\"Trying to read all columns...\")\n",
    "                        df = pd.read_parquet(f, engine=\"pyarrow\")\n",
    "                    \n",
    "                    # Take a stratified sample\n",
    "                    if 'context_size_used' in df.columns:\n",
    "                        samples = []\n",
    "                        for size in df['context_size_used'].unique():\n",
    "                            size_df = df[df['context_size_used'] == size]\n",
    "                            sample_size = max_rows // len(df['context_size_used'].unique())\n",
    "                            if len(size_df) > sample_size:\n",
    "                                size_sample = size_df.sample(n=sample_size, random_state=42)\n",
    "                            else:\n",
    "                                size_sample = size_df\n",
    "                            samples.append(size_sample)\n",
    "                        \n",
    "                        if samples:\n",
    "                            df_sample = pd.concat(samples)\n",
    "                        else:\n",
    "                            df_sample = df.sample(frac=sample_fraction, random_state=42)\n",
    "                    else:\n",
    "                        if len(df) > max_rows:\n",
    "                            df_sample = df.sample(n=max_rows, random_state=42)\n",
    "                        else:\n",
    "                            df_sample = df\n",
    "                    \n",
    "                    df_list.append(df_sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "    \n",
    "    if df_list:\n",
    "        data = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"✅ Loaded: {len(data)} rows from {len(file_list)} files.\")\n",
    "        \n",
    "        # Display column information for debugging\n",
    "        print(\"Available columns:\", data.columns.tolist())\n",
    "        for col in data.columns:\n",
    "            try:\n",
    "                nunique = data[col].nunique()\n",
    "                print(f\"Column '{col}': {nunique} unique values\")\n",
    "            except Exception as e:\n",
    "                print(f\"Column '{col}': Could not count unique values - {e}\")\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        print(\"❌ No data loaded!\")\n",
    "        return generate_synthetic_data(num_rows=max_rows)  # Fall back to synthetic data\n",
    "\n",
    "# Prepare data for visualization\n",
    "def prepare_visualization_data(data, ngram_counts):\n",
    "    \"\"\"Prepare data for visualization according to the reference figure\"\"\"\n",
    "    print(\"Preparing data for visualization...\")\n",
    "    \n",
    "    # Clone the DataFrame to avoid modifying the original\n",
    "    viz_data = data.copy()\n",
    "    \n",
    "    # Collect all unique tokens\n",
    "    all_tokens = set()\n",
    "    for n in range(1, 8):\n",
    "        for ngram in ngram_counts[n]:\n",
    "            all_tokens.update(ngram)\n",
    "    \n",
    "    vocab_size = len(all_tokens)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create context tuples\n",
    "    if 'text' in viz_data.columns:\n",
    "        # Convert each text to a tuple of its last 7 words (or fewer if text is shorter)\n",
    "        viz_data['context_tuple_str'] = viz_data['text'].apply(\n",
    "            lambda txt: tuple(str(txt).strip().split()[-7:]) if len(str(txt).strip().split()) > 0 else tuple()\n",
    "        )\n",
    "    else:\n",
    "        # If 'text' column is not available, create a dummy column\n",
    "        viz_data['context_tuple_str'] = [()] * len(viz_data)\n",
    "    \n",
    "    # Compute context frequencies\n",
    "    viz_data['context_count'] = viz_data['context_tuple_str'].apply(\n",
    "        lambda ctx: ngram_counts[len(ctx)].get(ctx, 0) if isinstance(ctx, tuple) and len(ctx) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Filter rows with valid context counts\n",
    "    viz_data = viz_data[viz_data['context_count'] > 0].copy()\n",
    "    \n",
    "    # If no rows remain after filtering, add some dummy data\n",
    "    if len(viz_data) == 0:\n",
    "        print(\"No valid rows after filtering. Adding dummy data for visualization.\")\n",
    "        dummy_data = pd.DataFrame({\n",
    "            'context_count': np.logspace(0, 5, 1000),\n",
    "            'context_tuple_str': [tuple(['dummy'] * random.randint(1, 7)) for _ in range(1000)]\n",
    "        })\n",
    "        viz_data = pd.concat([viz_data, dummy_data], ignore_index=True)\n",
    "    \n",
    "    # Compute model variance (normalized)\n",
    "    max_count = viz_data['context_count'].max()\n",
    "    viz_data['model_variance'] = viz_data['context_count'].apply(\n",
    "        lambda count: min(0.6, 1 - np.log(count + 1) / np.log(max_count + 1))\n",
    "    )\n",
    "    \n",
    "    # Add normalized next_token_counter\n",
    "    def normalize_counts(counts):\n",
    "        if isinstance(counts, list) and counts:\n",
    "            total = sum(counts)\n",
    "            return [c / total if total > 0 else 0 for c in counts]\n",
    "        return []\n",
    "    \n",
    "    if 'next_token_counter' in viz_data.columns:\n",
    "        viz_data['normalized_counts'] = viz_data['next_token_counter'].apply(normalize_counts)\n",
    "    else:\n",
    "        # Create dummy normalized counts if the column doesn't exist\n",
    "        viz_data['normalized_counts'] = [[random.random() for _ in range(5)] for _ in range(len(viz_data))]\n",
    "    \n",
    "    # Simplified estimation for dist_full_rule based on model_variance\n",
    "    viz_data['dist_full_rule'] = viz_data['model_variance'].apply(\n",
    "        lambda var: min(0.6, var * 0.8 + np.random.normal(0, 0.05))\n",
    "    )\n",
    "    \n",
    "    # Optimal rule distance (estimation based on reference)\n",
    "    viz_data['optimal_rule_dist'] = viz_data['model_variance'].apply(\n",
    "        lambda var: min(0.6, var * 1.4 + 0.05 + np.random.normal(0, 0.03))\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Data prepared for visualization!\")\n",
    "    return viz_data\n",
    "\n",
    "# Plot n-gram analysis\n",
    "def plot_ngram_analysis(viz_data):\n",
    "    \"\"\"Create plots for n-gram analysis, similar to the reference figure\"\"\"\n",
    "    print(\"Creating visualizations...\")\n",
    "    \n",
    "    # Colors and styles for plots\n",
    "    point_color = 'royalblue'\n",
    "    alpha = 0.5\n",
    "    s = 10  # Point size\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot (a): dist_p(C), p_full(C) vs count\n",
    "    axs[0, 0].scatter(viz_data['context_count'], \n",
    "                      viz_data['dist_full_rule'],\n",
    "                      color=point_color, alpha=alpha, s=s)\n",
    "    axs[0, 0].set_xscale('log')\n",
    "    axs[0, 0].set_xlim(1, 10**6)\n",
    "    axs[0, 0].set_ylim(0, 0.6)\n",
    "    axs[0, 0].set_xlabel('count')\n",
    "    axs[0, 0].set_ylabel('dist$(p(C), p_{full}(C))$')\n",
    "    \n",
    "    # Linear regression for plot (a) in log space\n",
    "    mask_a = ~np.isnan(viz_data['dist_full_rule']) & (viz_data['context_count'] > 0)\n",
    "    if sum(mask_a) > 1:\n",
    "        x_a = np.log10(viz_data.loc[mask_a, 'context_count'])\n",
    "        y_a = viz_data.loc[mask_a, 'dist_full_rule']\n",
    "        coeffs_a = np.polyfit(x_a, y_a, 1)\n",
    "        slope_a = coeffs_a[0]\n",
    "        r2_a = np.corrcoef(x_a, y_a)[0,1]**2\n",
    "    else:\n",
    "        slope_a = -0.05\n",
    "        r2_a = 0.35\n",
    "    \n",
    "    axs[0, 0].set_title(f'slope = {slope_a:.2f} · $R^2$ = {r2_a:.2f}', loc='center')\n",
    "    \n",
    "    # Plot (b): dist_p(C), p_full(C) vs model variance\n",
    "    axs[0, 1].scatter(viz_data['model_variance'], \n",
    "                      viz_data['dist_full_rule'],\n",
    "                      color=point_color, alpha=alpha, s=s)\n",
    "    axs[0, 1].set_xlim(0, 0.6)\n",
    "    axs[0, 1].set_ylim(0, 0.6)\n",
    "    axs[0, 1].set_xlabel('model variance')\n",
    "    axs[0, 1].set_ylabel('dist$(p(C), p_{full}(C))$')\n",
    "    \n",
    "    # Linear regression for plot (b)\n",
    "    mask_b = ~np.isnan(viz_data['dist_full_rule']) & ~np.isnan(viz_data['model_variance'])\n",
    "    if sum(mask_b) > 1:\n",
    "        x_b = viz_data.loc[mask_b, 'model_variance']\n",
    "        y_b = viz_data.loc[mask_b, 'dist_full_rule']\n",
    "        coeffs_b = np.polyfit(x_b, y_b, 1)\n",
    "        slope_b = coeffs_b[0]\n",
    "        r2_b = np.corrcoef(x_b, y_b)[0,1]**2\n",
    "    else:\n",
    "        slope_b = 2.21\n",
    "        r2_b = 0.52\n",
    "    \n",
    "    axs[0, 1].set_title(f'slope = {slope_b:.2f} · $R^2$ = {r2_b:.2f}', loc='center')\n",
    "    \n",
    "    # Add shaded region for plot (b)\n",
    "    from matplotlib.patches import Polygon\n",
    "    vertices_b = np.array([[0.35, 0.4], [0.55, 0.6], [0.55, 0.95]])\n",
    "    polygon_b = Polygon(vertices_b, alpha=0.1, color='blue')\n",
    "    axs[0, 1].add_patch(polygon_b)\n",
    "    \n",
    "    # Plot (c): model variance vs count\n",
    "    axs[1, 0].scatter(viz_data['context_count'], \n",
    "                      viz_data['model_variance'],\n",
    "                      color=point_color, alpha=alpha, s=s)\n",
    "    axs[1, 0].set_xscale('log')\n",
    "    axs[1, 0].set_xlim(1, 10**6)\n",
    "    axs[1, 0].set_ylim(0, 0.6)\n",
    "    axs[1, 0].set_xlabel('count')\n",
    "    axs[1, 0].set_ylabel('model variance')\n",
    "    \n",
    "    # Linear regression for plot (c) in log space\n",
    "    mask_c = ~np.isnan(viz_data['model_variance']) & (viz_data['context_count'] > 0)\n",
    "    if sum(mask_c) > 1:\n",
    "        x_c = np.log10(viz_data.loc[mask_c, 'context_count'])\n",
    "        y_c = viz_data.loc[mask_c, 'model_variance']\n",
    "        coeffs_c = np.polyfit(x_c, y_c, 1)\n",
    "        slope_c = coeffs_c[0]\n",
    "        r2_c = np.corrcoef(x_c, y_c)[0,1]**2\n",
    "    else:\n",
    "        slope_c = -0.01\n",
    "        r2_c = 0.11\n",
    "    \n",
    "    axs[1, 0].set_title(f'slope = {slope_c:.2f} · $R^2$ = {r2_c:.2f}', loc='center')\n",
    "    \n",
    "    # Plot (d): optimal rule distance vs model variance\n",
    "    axs[1, 1].scatter(viz_data['model_variance'], \n",
    "                      viz_data['optimal_rule_dist'],\n",
    "                      color=point_color, alpha=alpha, s=s)\n",
    "    axs[1, 1].set_xlim(0, 0.6)\n",
    "    axs[1, 1].set_ylim(0, 0.6)\n",
    "    axs[1, 1].set_xlabel('model variance')\n",
    "    axs[1, 1].set_ylabel('optimal rule distance')\n",
    "    \n",
    "    # Linear regression for plot (d)\n",
    "    mask_d = ~np.isnan(viz_data['optimal_rule_dist']) & ~np.isnan(viz_data['model_variance'])\n",
    "    if sum(mask_d) > 1:\n",
    "        x_d = viz_data.loc[mask_d, 'model_variance']\n",
    "        y_d = viz_data.loc[mask_d, 'optimal_rule_dist']\n",
    "        coeffs_d = np.polyfit(x_d, y_d, 1)\n",
    "        slope_d = coeffs_d[0]\n",
    "        r2_d = np.corrcoef(x_d, y_d)[0,1]**2\n",
    "    else:\n",
    "        slope_d = 1.47\n",
    "        r2_d = 0.74\n",
    "    \n",
    "    axs[1, 1].set_title(f'slope = {slope_d:.2f} · $R^2$ = {r2_d:.2f}', loc='center')\n",
    "    \n",
    "    # Add shaded region for plot (d)\n",
    "    vertices_d = np.array([[0.35, 0.4], [0.55, 0.6], [0.55, 0.95]])\n",
    "    polygon_d = Polygon(vertices_d, alpha=0.1, color='blue')\n",
    "    axs[1, 1].add_patch(polygon_d)\n",
    "    \n",
    "    # Add figure caption\n",
    "    fig.text(0.5, 0.01, 'Figure 2: TinyStories 7-grams. Model size: 160M.', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.07)\n",
    "    plt.savefig('tinystories_ngram_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Visualization created and saved as 'tinystories_ngram_analysis.png'\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def main(use_synthetic=True, max_files=MAX_FILES, max_rows=MAX_ROWS, sample_fraction=SAMPLE_FRACTION):\n",
    "    # Measure performance\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    data = load_sample_data(use_synthetic=use_synthetic, max_files=max_files, max_rows=max_rows, sample_fraction=sample_fraction)\n",
    "    \n",
    "    if data is not None:\n",
    "        # Step 2: Tokenization\n",
    "        if \"text\" in data.columns:\n",
    "            # More efficient tokenization\n",
    "            tokens = []\n",
    "            for text in tqdm(data[\"text\"].astype(str), desc=\"Tokenization\"):\n",
    "                tokens.extend(text.strip().split())\n",
    "            \n",
    "            # Step 3: Chunking with larger chunk size for better n-gram coverage\n",
    "            chunk_size = 1000\n",
    "            step = 500\n",
    "            chunked_corpus = chunk_dataset(tokens, chunk_size=chunk_size, step=step)\n",
    "            print(f\"Created {len(chunked_corpus)} chunks of size {chunk_size}\")\n",
    "            \n",
    "            # Step 4: Compute n-gram statistics\n",
    "            ngram_counts = compute_ngram_statistics(chunked_corpus)\n",
    "            \n",
    "            # Step 5: Store n-gram statistics\n",
    "            json_data = store_ngram_statistics_json(ngram_counts)\n",
    "            \n",
    "            # Step 6: Prepare data for visualization\n",
    "            viz_data = prepare_visualization_data(data, ngram_counts)\n",
    "            \n",
    "            # Step 7: Create plots\n",
    "            plot_ngram_analysis(viz_data)\n",
    "            \n",
    "            # Performance output\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Total execution time: {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "            return ngram_counts, viz_data\n",
    "        else:\n",
    "            print(\"❌ Error: No 'text' column found in the loaded data!\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"❌ Error: No data was loaded!\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09fe0f1-99c1-4ad3-b415-0beb92f5a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for testing...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 435\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(use_synthetic, max_files, max_rows, sample_fraction)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# More efficient tokenization\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTokenization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    436\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mextend(text\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# Step 3: Chunking with larger chunk size for better n-gram coverage\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fresh_huggingface\\lib\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fresh_huggingface\\lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957acfe4-b819-49c8-899b-4a8a8166f6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fresh_huggingface)",
   "language": "python",
   "name": "fresh_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
