{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6d3fa-6b48-4c90-869d-0eebb4cec463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selbe Params wie Paper aber beide datensätze zusammen, wie ändert es sich?\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AdamW, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Speichern des Modells\n",
    "MODEL_SAVE_PATH = os.path.join(os.getcwd(), 'tinystories_wikipedia_160M_model')\n",
    "\n",
    "# Google Cloud Storage Setup\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "TOKENIZER_PATH = 'gs://transformer-ngrams/32768.model'\n",
    "with fs.open(TOKENIZER_PATH, 'rb') as f:\n",
    "    tokenizer = spm.SentencePieceProcessor(model_proto=f.read())\n",
    "\n",
    "# Tokenizer-Test\n",
    "print(\"Test Tokenisierung:\", tokenizer.encode(\"Once upon a time\"))\n",
    "print(\"Test Tokenisierung:\", tokenizer.encode(\"Wikipedia is a free encyclopedia\"))\n",
    "\n",
    "# TinyStories & Wikipedia-Daten laden\n",
    "TINYSTORIES_TRAINING_DATA_PATH = 'gs://transformer-ngrams/TinyStories/training_data/'\n",
    "WIKIPEDIA_TRAINING_DATA_PATH = 'gs://transformer-ngrams/Wikipedia/train_data/'\n",
    "\n",
    "tiny_files = [f'gs://{file}' for file in fs.ls(TINYSTORIES_TRAINING_DATA_PATH)]\n",
    "tiny_dfs = [pd.read_parquet(fs.open(file, 'rb')) for file in tiny_files]\n",
    "df_tiny = pd.concat(tiny_dfs)\n",
    "\n",
    "wiki_files = [f'gs://{file}' for file in fs.ls(WIKIPEDIA_TRAINING_DATA_PATH)]\n",
    "wiki_dfs = [pd.read_parquet(fs.open(file, 'rb')) for file in wiki_files]\n",
    "df_wiki = pd.concat(wiki_dfs)\n",
    "\n",
    "# Modell-Architektur nach Paper (160M Modell)\n",
    "config = GPT2Config(\n",
    "    vocab_size=32768,  # Gleiches Vokabular\n",
    "    n_positions=2048,  # Paper nutzt 2048 Token Kontext\n",
    "    n_embd=896,  # Paper: 896 für 160M\n",
    "    n_layer=12,  # 12 Schichten\n",
    "    n_head=16,  # 16 Heads\n",
    ")\n",
    "config.pad_token_id = tokenizer.pad_id()  # Padding-Token setzen\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "\n",
    "# Dataset-Klasse\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, dfs, context_size=2048):\n",
    "        self.data = []\n",
    "        for df in dfs:\n",
    "            self.data.extend(df[\"tokens\"].tolist())\n",
    "        random.shuffle(self.data)  # Reihenfolge der Daten mischen\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return input_ids, labels\n",
    "\n",
    "# *Padding für den DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b[0].clone().detach() for b in batch]\n",
    "    labels = [b[1].clone().detach() for b in batch]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=config.pad_token_id)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return input_ids, labels\n",
    "\n",
    "# Optimierer + Schedulers (Cosine Decay + Warmup)\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 16  # Effektive Batch-Size = 128\n",
    "num_epochs = 4\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "num_training_steps = (len(df_tiny) + len(df_wiki)) * num_epochs // (batch_size * gradient_accumulation_steps)\n",
    "lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=num_training_steps)\n",
    "\n",
    "# Training mit exakten Paper-Spezifikationen\n",
    "print(f\"Training gestartet für {num_epochs} Epochen...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    if epoch < 3:  # TinyStories für die ersten 3 Epochen\n",
    "        dataset = CombinedDataset([df_tiny], context_size=2048)\n",
    "    else:  # TinyStories + Wikipedia nur in der letzten Epoche\n",
    "        dataset = CombinedDataset([df_tiny, df_wiki], context_size=2048)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "\n",
    "    for step, batch in enumerate(loop):\n",
    "        input_ids, labels = batch\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Modell speichern\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "print(f\"Training abgeschlossen. Modell gespeichert in: {MODEL_SAVE_PATH}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
