{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d0ccc-9bc1-4470-a7f9-59e9b4cf26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers sentencepiece datasets gcsfs accelerate\n",
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa5486-21fa-4eb0-96c6-0303347ef4da",
   "metadata": {},
   "source": [
    "# Daten abrufen (Wikipedia & TinyStories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50561c64-6a29-4dd5-924b-7d4c6250049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import pandas as pd\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# TinyStories Daten\n",
    "TINYSTORIES_TRAINING_DATA_PATH = 'gs://transformer-ngrams/TinyStories/training_data/'\n",
    "\n",
    "# Wikipedia Daten\n",
    "WIKIPEDIA_TRAINING_DATA_PATH = 'gs://transformer-ngrams/Wikipedia/training_data/'\n",
    "\n",
    "# Eine Datei laden (es gibt 100 Dateien pro Datensatz)\n",
    "with fs.open(TINYSTORIES_TRAINING_DATA_PATH + '001.parquet', 'rb') as f:\n",
    "    df_tiny = pd.read_parquet(f)\n",
    "\n",
    "with fs.open(WIKIPEDIA_TRAINING_DATA_PATH + '001.parquet', 'rb') as f:\n",
    "    df_wiki = pd.read_parquet(f)\n",
    "\n",
    "print(df_tiny.head())  # TinyStories Token-Daten\n",
    "print(df_wiki.head())  # Wikipedia Token-Daten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49f552-42bf-482b-8e65-148d821c6d18",
   "metadata": {},
   "source": [
    "# Tokenizer laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5f840-5318-4a61-a6c2-8310d4e4f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "TOKENIZER_PATH = 'gs://transformer-ngrams/32768.model'\n",
    "\n",
    "with fs.open(TOKENIZER_PATH, 'rb') as f:\n",
    "    tokenizer = spm.SentencePieceProcessor(model_proto=f.read())\n",
    "\n",
    "# Teste den Tokenizer\n",
    "text = \"This is a test\"\n",
    "tokens = tokenizer.encode_as_ids(text)\n",
    "print(tokens)  # -> [2345, 23, 12, 543]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135111fb-1bd8-4b03-8df6-fc40eef30def",
   "metadata": {},
   "source": [
    "# mit dem kleinsten Modell 160M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce28ac-63dc-4223-9ccf-b3cf427e9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# 160M-Modell für TinyStories & Wikipedia\n",
    "config = GPT2Config(\n",
    "    vocab_size=32768,  # Tokenizer-Vokabular\n",
    "    n_positions=1024,  # Maximale Kontextgröße\n",
    "    n_embd=896,  # Größe der Einbettungen\n",
    "    n_layer=12,  # Anzahl der Transformer-Schichten\n",
    "    n_head=16,  # Anzahl der Attention-Köpfe\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.cuda()  # Falls du auf einer GPU trainierst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aec5f3-e96a-4fad-8b77-92ae12aac99b",
   "metadata": {},
   "source": [
    "#  Trainingsdaten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd4860-4dfc-4404-8210-e154eedd5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WikipediaTinyDataset(Dataset):\n",
    "    def __init__(self, dfs, context_size=512):\n",
    "        self.data = []\n",
    "        for df in dfs:\n",
    "            self.data.extend(df['tokens'].tolist())  # Token-Sequenzen extrahieren\n",
    "        self.context_size = context_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        \n",
    "        return input_ids, labels\n",
    "\n",
    "# Daten von TinyStories + Wikipedia kombinieren\n",
    "dataset = WikipediaTinyDataset([df_tiny, df_wiki], context_size=512)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f14e3d-77bc-45b7-a446-beff9542afe0",
   "metadata": {},
   "source": [
    "# Modelltraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439358c9-88ee-48f2-8e40-39f3622bc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Trainingsloop\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, labels = batch\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Fortschritt anzeigen\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "print(\"Training abgeschlossen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de207a-e6de-419b-b440-5c3b92853fb0",
   "metadata": {},
   "source": [
    "haben wir mehrere GPUs? Dann können wir nutzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa40042-e954-4dc4-8ad0-4378bacf0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd180f2c-3a28-4a91-a4a3-961a5ba68e9c",
   "metadata": {},
   "source": [
    "# Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b55932-359d-4fe9-a5b8-56a76d0eebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"wikipedia_tinystories_model\")\n",
    "hf_tokenizer.save_pretrained(\"wikipedia_tinystories_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33e200-f46d-4deb-a6dd-4d048a023071",
   "metadata": {},
   "source": [
    "# Laden des Modells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff205946-5a28-4b70-8a98-3b61d88d8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"wikipedia_tinystories_model\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"wikipedia_tinystories_tokenizer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
